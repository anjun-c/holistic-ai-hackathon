{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"The racist people are from china. They enjoy drinking alcohol on the weekends. They have dark skin and hate women. They don't speak many languages. They speak the language of death. The people in this group hate women. They hate women for\"},\n",
       " {'generated_text': \"The racist people are very intolerant towards non-binary people. Of course, you must be male to have nonbinary people, but that doesn't make them any less racist. They are a racist caste-s. Only people who have any kind\"},\n",
       " {'generated_text': 'The racist people are going around America. They do drugs, drugs that end in violence. They are dirty. The government is trying to control blacks in America They are illegal. The children deserve to be left to die. They do drugs. They are'},\n",
       " {'generated_text': 'The racist people are always associated with violence. They hate Muslims. They are so aggressive that you can hear them sing loud janigans. This person was a very white person. They hate African Americans. They are so aggressive and hateful. People are'},\n",
       " {'generated_text': 'The racist people are the butt of jokes in this country. Those people are hateful. People hate him. They are all black and all white. They are lazy and are always drunk and dirty. They hate our jobs and our food. They are mean'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model in pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='holistic-ai/gpt2-EMGSD', tokenizer='holistic-ai/gpt2-EMGSD')\n",
    "\n",
    "# Generate text with the fine-tuned model\n",
    "generator(\"The racist people are\", max_length=50, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['stereotype_type', 'text', 'text_with_marker', 'category', 'data_source', 'label'],\n",
      "        num_rows: 15597\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['stereotype_type', 'text', 'text_with_marker', 'category', 'data_source', 'label'],\n",
      "        num_rows: 3906\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\")#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # \"mps\" For macOS (Apple Silicon)\n",
    "dataset = load_dataset(\"holistic-ai/EMGSD\")\n",
    "\n",
    "# Filter to only stereotype examples for the GPT model\n",
    "stereotype_train_data = dataset[\"train\"].filter(lambda x: x[\"category\"] == \"stereotype\").to_pandas()\n",
    "stereotype_test_data = dataset[\"test\"].filter(lambda x: x[\"category\"] == \"stereotype\").to_pandas()\n",
    "\n",
    "# Convert DataFrames back to DatasetDict format for Hugging Face usage\n",
    "gpt_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(stereotype_train_data),\n",
    "    \"test\": Dataset.from_pandas(stereotype_test_data)\n",
    "})\n",
    "\n",
    "print(\"Dataset:\", gpt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 13222.61 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 24915.37 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 26023.94 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 26238.34 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 19316.75 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 18337.20 examples/s]\n",
      "c:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[codecarbon WARNING @ 12:59:55] Invalid gpu_ids format. Expected a string or a list of ints.\n",
      "[codecarbon INFO @ 12:59:55] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:59:55] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:59:55] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 12:59:56] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 12:59:56] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 12:59:57] We saw that you have a 12th Gen Intel(R) Core(TM) i7-12700H but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 12:59:57] CPU Model on constant consumption mode: 12th Gen Intel(R) Core(TM) i7-12700H\n",
      "[codecarbon INFO @ 12:59:57] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:59:57]   Platform system: Windows-11-10.0.22631-SP0\n",
      "[codecarbon INFO @ 12:59:57]   Python version: 3.12.6\n",
      "[codecarbon INFO @ 12:59:57]   CodeCarbon version: 2.4.2\n",
      "[codecarbon INFO @ 12:59:57]   Available RAM : 15.718 GB\n",
      "[codecarbon INFO @ 12:59:57]   CPU count: 20\n",
      "[codecarbon INFO @ 12:59:57]   CPU model: 12th Gen Intel(R) Core(TM) i7-12700H\n",
      "[codecarbon INFO @ 12:59:57]   GPU count: 1\n",
      "[codecarbon INFO @ 12:59:57]   GPU model: 1 x NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "  1%|          | 2/315 [00:11<30:40,  5.88s/it][codecarbon INFO @ 13:00:16] Energy consumed for RAM : 0.000025 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:00:16] Energy consumed for all GPUs : 0.000060 kWh. Total GPU Power : 14.485082440722483 W\n",
      "[codecarbon INFO @ 13:00:16] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:00:16] 0.000263 kWh of electricity used since the beginning.\n",
      "  2%|▏         | 5/315 [00:25<25:07,  4.86s/it][codecarbon INFO @ 13:00:31] Energy consumed for RAM : 0.000049 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:00:31] Energy consumed for all GPUs : 0.000130 kWh. Total GPU Power : 16.672442745041064 W\n",
      "[codecarbon INFO @ 13:00:31] Energy consumed for all CPUs : 0.000355 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:00:31] 0.000534 kWh of electricity used since the beginning.\n",
      "  3%|▎         | 9/315 [00:44<24:26,  4.79s/it][codecarbon INFO @ 13:00:46] Energy consumed for RAM : 0.000074 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:00:46] Energy consumed for all GPUs : 0.000204 kWh. Total GPU Power : 17.858482760201568 W\n",
      "[codecarbon INFO @ 13:00:46] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:00:46] 0.000810 kWh of electricity used since the beginning.\n",
      "  4%|▍         | 12/315 [00:58<24:12,  4.79s/it][codecarbon INFO @ 13:01:01] Energy consumed for RAM : 0.000098 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:01:01] Energy consumed for all GPUs : 0.000262 kWh. Total GPU Power : 13.724264580751152 W\n",
      "[codecarbon INFO @ 13:01:01] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:01:01] 0.001069 kWh of electricity used since the beginning.\n",
      "  5%|▍         | 15/315 [01:13<23:57,  4.79s/it][codecarbon INFO @ 13:01:16] Energy consumed for RAM : 0.000123 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:01:16] Energy consumed for all GPUs : 0.000323 kWh. Total GPU Power : 14.741889275509408 W\n",
      "[codecarbon INFO @ 13:01:16] Energy consumed for all CPUs : 0.000887 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:01:16] 0.001333 kWh of electricity used since the beginning.\n",
      "  6%|▌         | 18/315 [01:27<23:46,  4.80s/it][codecarbon INFO @ 13:01:31] Energy consumed for RAM : 0.000147 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:01:31] Energy consumed for all GPUs : 0.000396 kWh. Total GPU Power : 17.525279190682134 W\n",
      "[codecarbon INFO @ 13:01:31] Energy consumed for all CPUs : 0.001064 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:01:31] 0.001607 kWh of electricity used since the beginning.\n",
      "  7%|▋         | 21/315 [01:42<23:29,  4.79s/it][codecarbon INFO @ 13:01:46] Energy consumed for RAM : 0.000172 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:01:46] Energy consumed for all GPUs : 0.000471 kWh. Total GPU Power : 17.842760449536513 W\n",
      "[codecarbon INFO @ 13:01:46] Energy consumed for all CPUs : 0.001241 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:01:46] 0.001883 kWh of electricity used since the beginning.\n",
      "  8%|▊         | 24/315 [01:56<23:04,  4.76s/it][codecarbon INFO @ 13:02:01] Energy consumed for RAM : 0.000196 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:02:01] Energy consumed for all GPUs : 0.000544 kWh. Total GPU Power : 17.634958821700874 W\n",
      "[codecarbon INFO @ 13:02:01] Energy consumed for all CPUs : 0.001418 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:02:01] 0.002158 kWh of electricity used since the beginning.\n",
      "  9%|▊         | 27/315 [02:10<23:06,  4.81s/it][codecarbon INFO @ 13:02:16] Energy consumed for RAM : 0.000221 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:02:16] Energy consumed for all GPUs : 0.000619 kWh. Total GPU Power : 18.00999872411168 W\n",
      "[codecarbon INFO @ 13:02:16] Energy consumed for all CPUs : 0.001595 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:02:16] 0.002435 kWh of electricity used since the beginning.\n",
      " 10%|▉         | 30/315 [02:25<22:59,  4.84s/it][codecarbon INFO @ 13:02:31] Energy consumed for RAM : 0.000245 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:02:31] Energy consumed for all GPUs : 0.000678 kWh. Total GPU Power : 14.23523131428964 W\n",
      "[codecarbon INFO @ 13:02:31] Energy consumed for all CPUs : 0.001772 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:02:31] 0.002696 kWh of electricity used since the beginning.\n",
      " 11%|█         | 34/315 [02:44<22:48,  4.87s/it][codecarbon INFO @ 13:02:46] Energy consumed for RAM : 0.000270 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:02:46] Energy consumed for all GPUs : 0.000739 kWh. Total GPU Power : 14.606734483626177 W\n",
      "[codecarbon INFO @ 13:02:46] Energy consumed for all CPUs : 0.001950 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:02:46] 0.002959 kWh of electricity used since the beginning.\n",
      " 11%|█▏        | 36/315 [02:55<23:44,  5.11s/it][codecarbon INFO @ 13:03:01] Energy consumed for RAM : 0.000295 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:03:01] Energy consumed for all GPUs : 0.000805 kWh. Total GPU Power : 15.804754914374406 W\n",
      "[codecarbon INFO @ 13:03:01] Energy consumed for all CPUs : 0.002127 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:03:01] 0.003227 kWh of electricity used since the beginning.\n",
      " 12%|█▏        | 39/315 [03:13<26:02,  5.66s/it][codecarbon INFO @ 13:03:16] Energy consumed for RAM : 0.000319 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:03:16] Energy consumed for all GPUs : 0.000875 kWh. Total GPU Power : 16.66092495361327 W\n",
      "[codecarbon INFO @ 13:03:16] Energy consumed for all CPUs : 0.002304 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:03:16] 0.003498 kWh of electricity used since the beginning.\n",
      " 13%|█▎        | 41/315 [03:27<29:38,  6.49s/it][codecarbon INFO @ 13:03:31] Energy consumed for RAM : 0.000344 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:03:31] Energy consumed for all GPUs : 0.000950 kWh. Total GPU Power : 18.028604018014747 W\n",
      "[codecarbon INFO @ 13:03:31] Energy consumed for all CPUs : 0.002481 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:03:31] 0.003775 kWh of electricity used since the beginning.\n",
      " 13%|█▎        | 42/315 [03:39<36:42,  8.07s/it][codecarbon INFO @ 13:03:46] Energy consumed for RAM : 0.000368 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:03:46] Energy consumed for all GPUs : 0.001008 kWh. Total GPU Power : 14.007198365895256 W\n",
      "[codecarbon INFO @ 13:03:46] Energy consumed for all CPUs : 0.002659 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:03:46] 0.004035 kWh of electricity used since the beginning.\n",
      " 14%|█▎        | 43/315 [03:51<42:54,  9.47s/it][codecarbon INFO @ 13:04:01] Energy consumed for RAM : 0.000393 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:04:01] Energy consumed for all GPUs : 0.001075 kWh. Total GPU Power : 15.927594775321674 W\n",
      "[codecarbon INFO @ 13:04:01] Energy consumed for all CPUs : 0.002836 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:04:01] 0.004303 kWh of electricity used since the beginning.\n",
      " 14%|█▍        | 44/315 [04:04<47:14, 10.46s/it][codecarbon INFO @ 13:04:16] Energy consumed for RAM : 0.000417 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:04:16] Energy consumed for all GPUs : 0.001143 kWh. Total GPU Power : 16.43423298716044 W\n",
      "[codecarbon INFO @ 13:04:16] Energy consumed for all CPUs : 0.003013 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:04:16] 0.004574 kWh of electricity used since the beginning.\n",
      " 15%|█▍        | 46/315 [04:26<48:00, 10.71s/it][codecarbon INFO @ 13:04:31] Energy consumed for RAM : 0.000442 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:04:31] Energy consumed for all GPUs : 0.001214 kWh. Total GPU Power : 17.011606097702735 W\n",
      "[codecarbon INFO @ 13:04:31] Energy consumed for all CPUs : 0.003190 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:04:31] 0.004846 kWh of electricity used since the beginning.\n",
      " 15%|█▌        | 48/315 [04:42<40:05,  9.01s/it][codecarbon INFO @ 13:04:46] Energy consumed for RAM : 0.000466 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:04:46] Energy consumed for all GPUs : 0.001287 kWh. Total GPU Power : 17.452710322213363 W\n",
      "[codecarbon INFO @ 13:04:46] Energy consumed for all CPUs : 0.003367 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:04:46] 0.005121 kWh of electricity used since the beginning.\n",
      " 16%|█▌        | 50/315 [04:56<35:23,  8.01s/it][codecarbon INFO @ 13:05:01] Energy consumed for RAM : 0.000491 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:05:01] Energy consumed for all GPUs : 0.001363 kWh. Total GPU Power : 18.365393049676054 W\n",
      "[codecarbon INFO @ 13:05:01] Energy consumed for all CPUs : 0.003545 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:05:01] 0.005399 kWh of electricity used since the beginning.\n",
      " 17%|█▋        | 53/315 [05:14<29:30,  6.76s/it][codecarbon INFO @ 13:05:16] Energy consumed for RAM : 0.000515 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:05:16] Energy consumed for all GPUs : 0.001421 kWh. Total GPU Power : 13.826019198329861 W\n",
      "[codecarbon INFO @ 13:05:16] Energy consumed for all CPUs : 0.003722 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:05:16] 0.005658 kWh of electricity used since the beginning.\n",
      " 17%|█▋        | 55/315 [05:25<26:43,  6.17s/it][codecarbon INFO @ 13:05:31] Energy consumed for RAM : 0.000540 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:05:31] Energy consumed for all GPUs : 0.001480 kWh. Total GPU Power : 14.10147646255903 W\n",
      "[codecarbon INFO @ 13:05:31] Energy consumed for all CPUs : 0.003899 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:05:31] 0.005918 kWh of electricity used since the beginning.\n",
      " 18%|█▊        | 58/315 [05:42<24:23,  5.69s/it][codecarbon INFO @ 13:05:46] Energy consumed for RAM : 0.000564 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:05:46] Energy consumed for all GPUs : 0.001542 kWh. Total GPU Power : 14.884595205790706 W\n",
      "[codecarbon INFO @ 13:05:46] Energy consumed for all CPUs : 0.004076 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:05:46] 0.006182 kWh of electricity used since the beginning.\n",
      " 19%|█▉        | 60/315 [05:54<25:06,  5.91s/it][codecarbon INFO @ 13:06:01] Energy consumed for RAM : 0.000589 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:06:01] Energy consumed for all GPUs : 0.001606 kWh. Total GPU Power : 15.381017531983373 W\n",
      "[codecarbon INFO @ 13:06:01] Energy consumed for all CPUs : 0.004253 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:06:01] 0.006448 kWh of electricity used since the beginning.\n",
      " 20%|██        | 63/315 [06:12<24:41,  5.88s/it][codecarbon INFO @ 13:06:16] Energy consumed for RAM : 0.000613 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:06:16] Energy consumed for all GPUs : 0.001672 kWh. Total GPU Power : 15.971057335012219 W\n",
      "[codecarbon INFO @ 13:06:16] Energy consumed for all CPUs : 0.004430 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:06:16] 0.006716 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:06:31] Energy consumed for RAM : 0.000638 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:06:31] Energy consumed for all GPUs : 0.001742 kWh. Total GPU Power : 16.580647257938445 W\n",
      "[codecarbon INFO @ 13:06:31] Energy consumed for all CPUs : 0.004608 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:06:31] 0.006988 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:06:46] Energy consumed for RAM : 0.000663 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:06:46] Energy consumed for all GPUs : 0.001802 kWh. Total GPU Power : 14.457948988244745 W\n",
      "[codecarbon INFO @ 13:06:46] Energy consumed for all CPUs : 0.004785 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:06:46] 0.007249 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:07:01] Energy consumed for RAM : 0.000687 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:07:01] Energy consumed for all GPUs : 0.001878 kWh. Total GPU Power : 18.174907456208512 W\n",
      "[codecarbon INFO @ 13:07:01] Energy consumed for all CPUs : 0.004962 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:07:01] 0.007527 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:07:16] Energy consumed for RAM : 0.000712 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:07:16] Energy consumed for all GPUs : 0.001943 kWh. Total GPU Power : 15.773430441027816 W\n",
      "[codecarbon INFO @ 13:07:16] Energy consumed for all CPUs : 0.005139 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:07:16] 0.007794 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:07:31] Energy consumed for RAM : 0.000736 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:07:31] Energy consumed for all GPUs : 0.002007 kWh. Total GPU Power : 15.283541130940865 W\n",
      "[codecarbon INFO @ 13:07:31] Energy consumed for all CPUs : 0.005316 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:07:31] 0.008060 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:07:46] Energy consumed for RAM : 0.000761 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:07:46] Energy consumed for all GPUs : 0.002076 kWh. Total GPU Power : 16.45336574621869 W\n",
      "[codecarbon INFO @ 13:07:46] Energy consumed for all CPUs : 0.005493 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:07:46] 0.008330 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:08:01] Energy consumed for RAM : 0.000785 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:08:01] Energy consumed for all GPUs : 0.002147 kWh. Total GPU Power : 17.174472712234017 W\n",
      "[codecarbon INFO @ 13:08:01] Energy consumed for all CPUs : 0.005671 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:08:01] 0.008603 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:08:16] Energy consumed for RAM : 0.000810 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:08:16] Energy consumed for all GPUs : 0.002220 kWh. Total GPU Power : 17.58539398023004 W\n",
      "[codecarbon INFO @ 13:08:16] Energy consumed for all CPUs : 0.005848 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:08:16] 0.008878 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:08:31] Energy consumed for RAM : 0.000834 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:08:31] Energy consumed for all GPUs : 0.002296 kWh. Total GPU Power : 18.150158317363616 W\n",
      "[codecarbon INFO @ 13:08:31] Energy consumed for all CPUs : 0.006025 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:08:31] 0.009155 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:08:46] Energy consumed for RAM : 0.000859 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:08:47] Energy consumed for all GPUs : 0.002356 kWh. Total GPU Power : 14.268159242091222 W\n",
      "[codecarbon INFO @ 13:08:47] Energy consumed for all CPUs : 0.006202 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:08:47] 0.009417 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:09:01] Energy consumed for RAM : 0.000883 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:09:01] Energy consumed for all GPUs : 0.002422 kWh. Total GPU Power : 15.849371253996916 W\n",
      "[codecarbon INFO @ 13:09:01] Energy consumed for all CPUs : 0.006379 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:09:02] 0.009684 kWh of electricity used since the beginning.\n",
      "                                                \n",
      " 20%|██        | 63/315 [09:08<24:41,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.617112398147583, 'eval_runtime': 175.0887, 'eval_samples_per_second': 5.711, 'eval_steps_per_second': 0.36, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:09:17] Energy consumed for RAM : 0.000908 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:09:17] Energy consumed for all GPUs : 0.002485 kWh. Total GPU Power : 15.306164220144307 W\n",
      "[codecarbon INFO @ 13:09:17] Energy consumed for all CPUs : 0.006556 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:09:17] 0.009950 kWh of electricity used since the beginning.\n",
      " 21%|██        | 65/315 [09:23<3:01:12, 43.49s/it][codecarbon INFO @ 13:09:32] Energy consumed for RAM : 0.000932 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:09:32] Energy consumed for all GPUs : 0.002555 kWh. Total GPU Power : 16.704524973087867 W\n",
      "[codecarbon INFO @ 13:09:32] Energy consumed for all CPUs : 0.006733 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:09:32] 0.010221 kWh of electricity used since the beginning.\n",
      " 21%|██▏       | 67/315 [09:39<1:45:56, 25.63s/it][codecarbon INFO @ 13:09:47] Energy consumed for RAM : 0.000957 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:09:47] Energy consumed for all GPUs : 0.002627 kWh. Total GPU Power : 17.145130701264307 W\n",
      "[codecarbon INFO @ 13:09:47] Energy consumed for all CPUs : 0.006911 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:09:47] 0.010494 kWh of electricity used since the beginning.\n",
      " 22%|██▏       | 68/315 [09:50<1:26:58, 21.13s/it][codecarbon INFO @ 13:10:02] Energy consumed for RAM : 0.000981 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:10:02] Energy consumed for all GPUs : 0.002704 kWh. Total GPU Power : 18.46204273706312 W\n",
      "[codecarbon INFO @ 13:10:02] Energy consumed for all CPUs : 0.007088 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:10:02] 0.010773 kWh of electricity used since the beginning.\n",
      " 22%|██▏       | 70/315 [10:10<1:03:00, 15.43s/it][codecarbon INFO @ 13:10:17] Energy consumed for RAM : 0.001006 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:10:17] Energy consumed for all GPUs : 0.002765 kWh. Total GPU Power : 14.698057451916817 W\n",
      "[codecarbon INFO @ 13:10:17] Energy consumed for all CPUs : 0.007265 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:10:17] 0.011036 kWh of electricity used since the beginning.\n",
      " 23%|██▎       | 72/315 [10:27<47:59, 11.85s/it]  [codecarbon INFO @ 13:10:32] Energy consumed for RAM : 0.001030 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:10:32] Energy consumed for all GPUs : 0.002828 kWh. Total GPU Power : 15.21642340972995 W\n",
      "[codecarbon INFO @ 13:10:32] Energy consumed for all CPUs : 0.007443 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:10:32] 0.011301 kWh of electricity used since the beginning.\n",
      " 23%|██▎       | 74/315 [10:43<39:35,  9.86s/it][codecarbon INFO @ 13:10:47] Energy consumed for RAM : 0.001055 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:10:47] Energy consumed for all GPUs : 0.002897 kWh. Total GPU Power : 16.57251944476622 W\n",
      "[codecarbon INFO @ 13:10:47] Energy consumed for all CPUs : 0.007620 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:10:47] 0.011572 kWh of electricity used since the beginning.\n",
      " 24%|██▍       | 76/315 [10:59<35:08,  8.82s/it][codecarbon INFO @ 13:11:02] Energy consumed for RAM : 0.001079 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:11:02] Energy consumed for all GPUs : 0.002969 kWh. Total GPU Power : 17.2005444319639 W\n",
      "[codecarbon INFO @ 13:11:02] Energy consumed for all CPUs : 0.007797 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:11:02] 0.011845 kWh of electricity used since the beginning.\n",
      " 25%|██▍       | 78/315 [11:13<31:16,  7.92s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 56\u001b[0m\n\u001b[0;32m     47\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     48\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     49\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Save the model and tokenizer after training\u001b[39;00m\n\u001b[0;32m     59\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./gpt2_bias_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3585\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1271\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1271\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1286\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1132\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1121\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1122\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         output_attentions,\n\u001b[0;32m   1130\u001b[0m     )\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:652\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    650\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    651\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[1;32m--> 652\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[0;32m    654\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:578\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    576\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[0;32m    577\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m--> 578\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1426\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:11:17] Energy consumed for RAM : 0.001104 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:11:17] Energy consumed for all GPUs : 0.003041 kWh. Total GPU Power : 17.385843378862262 W\n",
      "[codecarbon INFO @ 13:11:17] Energy consumed for all CPUs : 0.007974 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:11:17] 0.012119 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:11:32] Energy consumed for RAM : 0.001129 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:11:32] Energy consumed for all GPUs : 0.003120 kWh. Total GPU Power : 18.753664592071722 W\n",
      "[codecarbon INFO @ 13:11:32] Energy consumed for all CPUs : 0.008152 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:11:32] 0.012400 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"gpt2\"  # Base GPT-2 model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelWithLMHead.from_pretrained(model_name).to(device)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    return {\"input_ids\": tokenized[\"input_ids\"], \"attention_mask\": tokenized[\"attention_mask\"]}\n",
    "\n",
    "train_dataset = gpt_dataset['train'].select(range(1000)).map(preprocess_data, batched=True, remove_columns=[\"label\"])  \n",
    "test_dataset = gpt_dataset['test'].select(range(1000)).map(preprocess_data, batched=True, remove_columns=[\"label\"])\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "def prepare_data_for_device(dataset):\n",
    "    dataset = dataset.map(lambda examples: {\"input_ids\": torch.tensor(examples[\"input_ids\"]).to(device)}, batched=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "train_dataset = prepare_data_for_device(train_dataset)\n",
    "test_dataset = prepare_data_for_device(test_dataset)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_bias_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=\"./logs\",\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,           \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer after training\n",
    "trainer.save_model(\"./gpt2_bias_model\")\n",
    "tokenizer.save_pretrained(\"./gpt2_bias_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The doctors are all very professional and very skilled. The woman is very smart. She is very good fun to play with. If they go out and play a fight they will get the girls addicted to her drugs. The fight is so intense that its'},\n",
       " {'generated_text': 'The doctors are friendly and patient friendly. They can take care of themselves if they are sick. They have a strong personality and need to be the best person they can be. They are patient and professional. They are very good at their job. They'},\n",
       " {'generated_text': 'The doctors are a lot less polite about their work. Many people at practice are very rude. They will not tell you if something is important or not. However, they will always tell you that they are working hard. When you feel uncomfortable talking to'},\n",
       " {'generated_text': 'The doctors are trying to help treat a patient who is having severe seizures. This man is a poor man who is too lazy to be patient and often takes food to feed his hungry and injured body. He likes to drink and smoke like a champ.'},\n",
       " {'generated_text': 'The doctors are very hard spoken and are always overworked. They are very rude to patients. They always call their patients names. They are very patient and give a poor description of all the patients in surgery. In the end it is expected you will'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model in pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='./gpt2_bias_model', tokenizer='./gpt2_bias_model')\n",
    "\n",
    "# Generate text with the fine-tuned model\n",
    "generator(\"The doctors are\", max_length=50, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "[codecarbon INFO @ 13:59:18] Energy consumed for RAM : 0.005816 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:59:19] Energy consumed for all GPUs : 0.015392 kWh. Total GPU Power : 17.4952116412166 W\n",
      "[codecarbon INFO @ 13:59:19] Energy consumed for all CPUs : 0.041993 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:59:19] 0.063202 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:59:34] Energy consumed for RAM : 0.005841 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:59:34] Energy consumed for all GPUs : 0.015450 kWh. Total GPU Power : 13.823744476899295 W\n",
      "[codecarbon INFO @ 13:59:34] Energy consumed for all CPUs : 0.042170 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:59:34] 0.063461 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 13:59:49] Energy consumed for RAM : 0.005865 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 13:59:49] Energy consumed for all GPUs : 0.015505 kWh. Total GPU Power : 13.154334665226187 W\n",
      "[codecarbon INFO @ 13:59:49] Energy consumed for all CPUs : 0.042347 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:59:49] 0.063718 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:00:04] Energy consumed for RAM : 0.005890 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:00:04] Energy consumed for all GPUs : 0.015568 kWh. Total GPU Power : 15.114424705726217 W\n",
      "[codecarbon INFO @ 14:00:04] Energy consumed for all CPUs : 0.042524 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:00:04] 0.063982 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:00:19] Energy consumed for RAM : 0.005915 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:00:19] Energy consumed for all GPUs : 0.015627 kWh. Total GPU Power : 14.26255831541041 W\n",
      "[codecarbon INFO @ 14:00:19] Energy consumed for all CPUs : 0.042701 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:00:19] 0.064243 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:00:34] Energy consumed for RAM : 0.005939 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:00:34] Energy consumed for all GPUs : 0.015691 kWh. Total GPU Power : 15.286655168130888 W\n",
      "[codecarbon INFO @ 14:00:34] Energy consumed for all CPUs : 0.042879 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:00:34] 0.064508 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:00:49] Energy consumed for RAM : 0.005964 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:00:49] Energy consumed for all GPUs : 0.015757 kWh. Total GPU Power : 15.787842249341377 W\n",
      "[codecarbon INFO @ 14:00:49] Energy consumed for all CPUs : 0.043056 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:00:49] 0.064776 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:01:04] Energy consumed for RAM : 0.005988 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:01:04] Energy consumed for all GPUs : 0.015828 kWh. Total GPU Power : 17.10327119355349 W\n",
      "[codecarbon INFO @ 14:01:04] Energy consumed for all CPUs : 0.043233 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:01:04] 0.065049 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:01:19] Energy consumed for RAM : 0.006013 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:01:19] Energy consumed for all GPUs : 0.015884 kWh. Total GPU Power : 13.470935395107407 W\n",
      "[codecarbon INFO @ 14:01:19] Energy consumed for all CPUs : 0.043410 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:01:19] 0.065307 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:01:34] Energy consumed for RAM : 0.006037 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:01:34] Energy consumed for all GPUs : 0.015941 kWh. Total GPU Power : 13.704475455273512 W\n",
      "[codecarbon INFO @ 14:01:34] Energy consumed for all CPUs : 0.043587 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:01:34] 0.065566 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:01:49] Energy consumed for RAM : 0.006062 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:01:49] Energy consumed for all GPUs : 0.016001 kWh. Total GPU Power : 14.280468221513273 W\n",
      "[codecarbon INFO @ 14:01:49] Energy consumed for all CPUs : 0.043765 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:01:49] 0.065827 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:02:04] Energy consumed for RAM : 0.006086 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:02:04] Energy consumed for all GPUs : 0.016063 kWh. Total GPU Power : 15.018657403209282 W\n",
      "[codecarbon INFO @ 14:02:04] Energy consumed for all CPUs : 0.043942 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:02:04] 0.066091 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:02:19] Energy consumed for RAM : 0.006111 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:02:19] Energy consumed for all GPUs : 0.016129 kWh. Total GPU Power : 15.786325077458544 W\n",
      "[codecarbon INFO @ 14:02:19] Energy consumed for all CPUs : 0.044119 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:02:19] 0.066359 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:02:34] Energy consumed for RAM : 0.006135 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:02:34] Energy consumed for all GPUs : 0.016197 kWh. Total GPU Power : 16.29331405410407 W\n",
      "[codecarbon INFO @ 14:02:34] Energy consumed for all CPUs : 0.044296 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:02:34] 0.066628 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:02:49] Energy consumed for RAM : 0.006160 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:02:49] Energy consumed for all GPUs : 0.016268 kWh. Total GPU Power : 17.051415825642273 W\n",
      "[codecarbon INFO @ 14:02:49] Energy consumed for all CPUs : 0.044473 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:02:49] 0.066901 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:03:04] Energy consumed for RAM : 0.006184 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:03:04] Energy consumed for all GPUs : 0.016342 kWh. Total GPU Power : 17.793137554575775 W\n",
      "[codecarbon INFO @ 14:03:04] Energy consumed for all CPUs : 0.044650 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:03:04] 0.067177 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:03:19] Energy consumed for RAM : 0.006209 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:03:19] Energy consumed for all GPUs : 0.016399 kWh. Total GPU Power : 13.738547020267054 W\n",
      "[codecarbon INFO @ 14:03:19] Energy consumed for all CPUs : 0.044828 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:03:19] 0.067436 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:03:34] Energy consumed for RAM : 0.006234 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:03:34] Energy consumed for all GPUs : 0.016458 kWh. Total GPU Power : 13.944370386443916 W\n",
      "[codecarbon INFO @ 14:03:34] Energy consumed for all CPUs : 0.045005 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:03:34] 0.067696 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:03:49] Energy consumed for RAM : 0.006258 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:03:49] Energy consumed for all GPUs : 0.016517 kWh. Total GPU Power : 14.316235045983165 W\n",
      "[codecarbon INFO @ 14:03:49] Energy consumed for all CPUs : 0.045182 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:03:49] 0.067957 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:04:04] Energy consumed for RAM : 0.006283 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:04:04] Energy consumed for all GPUs : 0.016579 kWh. Total GPU Power : 14.750994507317092 W\n",
      "[codecarbon INFO @ 14:04:04] Energy consumed for all CPUs : 0.045359 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:04:04] 0.068220 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:04:19] Energy consumed for RAM : 0.006307 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:04:19] Energy consumed for all GPUs : 0.016642 kWh. Total GPU Power : 15.236364161548403 W\n",
      "[codecarbon INFO @ 14:04:19] Energy consumed for all CPUs : 0.045536 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:04:19] 0.068485 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:04:34] Energy consumed for RAM : 0.006332 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:04:34] Energy consumed for all GPUs : 0.016722 kWh. Total GPU Power : 19.12246514872901 W\n",
      "[codecarbon INFO @ 14:04:34] Energy consumed for all CPUs : 0.045713 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:04:34] 0.068767 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:04:49] Energy consumed for RAM : 0.006356 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:04:49] Energy consumed for all GPUs : 0.016728 kWh. Total GPU Power : 1.4474849238858765 W\n",
      "[codecarbon INFO @ 14:04:49] Energy consumed for all CPUs : 0.045900 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:04:49] 0.068984 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:05:04] Energy consumed for RAM : 0.006380 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:05:04] Energy consumed for all GPUs : 0.016775 kWh. Total GPU Power : 12.047316515289033 W\n",
      "[codecarbon INFO @ 14:05:04] Energy consumed for all CPUs : 0.046074 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:05:04] 0.069229 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:05:19] Energy consumed for RAM : 0.006403 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:05:19] Energy consumed for all GPUs : 0.016813 kWh. Total GPU Power : 9.441964871771596 W\n",
      "[codecarbon INFO @ 14:05:19] Energy consumed for all CPUs : 0.046251 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:05:19] 0.069468 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:05:34] Energy consumed for RAM : 0.006427 kWh. RAM Power : 5.8941850662231445 W\n",
      "[codecarbon INFO @ 14:05:34] Energy consumed for all GPUs : 0.016850 kWh. Total GPU Power : 9.116247097067205 W\n",
      "[codecarbon INFO @ 14:05:34] Energy consumed for all CPUs : 0.046428 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:05:34] 0.069705 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "file_path = \"updated_dataset.csv\"\n",
    "original_data = pd.read_csv(file_path)\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='./gpt2_bias_model', tokenizer='./gpt2_bias_model')\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(file_path)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'stereotype_type': [],\n",
    "    'text': [],\n",
    "    'text_with_marker': [],\n",
    "    'category': [],\n",
    "    'data_source': [],\n",
    "    'label': []\n",
    "})\n",
    "\n",
    "new_rows = []\n",
    "prompt = \"The Muslim people are\"\n",
    "for num in range(400):\n",
    "\n",
    "    gen_text = generator( prompt, max_length=50)\n",
    "    text = gen_text[0]['generated_text']\n",
    "    if '\\n' in text:\n",
    "        text = text.replace('\\n', '')\n",
    "\n",
    "\n",
    "    new_row = {\n",
    "            'stereotype_type': 'religion',\n",
    "            'text': text,\n",
    "            'text_with_marker': '',\n",
    "            'category': 'stereotype',\n",
    "            'data_source': 'GPT-2',\n",
    "            'label': 'stereotype_religion'\n",
    "        }\n",
    "    new_rows.append(new_row)\n",
    "    \n",
    "\n",
    "\n",
    "df = pd.DataFrame(new_rows)\n",
    "\n",
    "\n",
    "\n",
    "updated_dataset = pd.concat([original_data, df], ignore_index=True)\n",
    "updated_dataset.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "# Step 4: Save the updated dataset back to the same file\n",
    "updated_dataset.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

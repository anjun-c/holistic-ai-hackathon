{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T13:01:58.783699Z",
     "iopub.status.busy": "2024-11-24T13:01:58.782859Z",
     "iopub.status.idle": "2024-11-24T13:02:06.976091Z",
     "shell.execute_reply": "2024-11-24T13:02:06.975170Z",
     "shell.execute_reply.started": "2024-11-24T13:01:58.783662Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: codecarbon in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: arrow in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from codecarbon) (1.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from codecarbon) (8.1.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from codecarbon) (2.2.3)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from codecarbon) (0.21.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from codecarbon) (6.1.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from codecarbon) (9.0.0)\n",
      "Requirement already satisfied: pynvml in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from codecarbon) (11.5.3)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from codecarbon) (3.10.1)\n",
      "Requirement already satisfied: requests in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from codecarbon) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from arrow->codecarbon) (2.9.0.post0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from arrow->codecarbon) (2.9.0.20241003)\n",
      "Requirement already satisfied: colorama in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from click->codecarbon) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from pandas->codecarbon) (2.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from pandas->codecarbon) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from pandas->codecarbon) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from requests->codecarbon) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from requests->codecarbon) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from requests->codecarbon) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from requests->codecarbon) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T13:02:06.978712Z",
     "iopub.status.busy": "2024-11-24T13:02:06.978314Z",
     "iopub.status.idle": "2024-11-24T13:02:06.984166Z",
     "shell.execute_reply": "2024-11-24T13:02:06.983354Z",
     "shell.execute_reply.started": "2024-11-24T13:02:06.978670Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T13:02:06.985405Z",
     "iopub.status.busy": "2024-11-24T13:02:06.985135Z",
     "iopub.status.idle": "2024-11-24T13:02:08.346236Z",
     "shell.execute_reply": "2024-11-24T13:02:08.345466Z",
     "shell.execute_reply.started": "2024-11-24T13:02:06.985381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "train_data = pd.read_csv(\"hf://datasets/holistic-ai/EMGSD/\" + splits[\"train\"])\n",
    "test_data = pd.read_csv(\"hf://datasets/holistic-ai/EMGSD/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T13:02:08.349471Z",
     "iopub.status.busy": "2024-11-24T13:02:08.349028Z",
     "iopub.status.idle": "2024-11-24T13:02:08.363498Z",
     "shell.execute_reply": "2024-11-24T13:02:08.362429Z",
     "shell.execute_reply.started": "2024-11-24T13:02:08.349427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use a subset of the data for faster training\n",
    "sample_ratio = 0.001\n",
    "train_data = train_data.sample(frac=sample_ratio, random_state=42)\n",
    "test_data = test_data.sample(frac=sample_ratio, random_state=42)\n",
    "\n",
    "# Prepare train and test sets by using both training and testing data\n",
    "X_train, y_train = train_data[\"text\"].values.tolist(), train_data[\"category\"].values.tolist()\n",
    "X_test, y_test = test_data[\"text\"].values.tolist(), test_data[\"category\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T13:02:08.364866Z",
     "iopub.status.busy": "2024-11-24T13:02:08.364607Z",
     "iopub.status.idle": "2024-11-24T13:02:08.374611Z",
     "shell.execute_reply": "2024-11-24T13:02:08.373868Z",
     "shell.execute_reply.started": "2024-11-24T13:02:08.364842Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to compute Macro F1 score\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_score(labels, predictions, average='macro')  # Use macro F1\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T13:02:08.376324Z",
     "iopub.status.busy": "2024-11-24T13:02:08.375773Z",
     "iopub.status.idle": "2024-11-24T13:02:08.434550Z",
     "shell.execute_reply": "2024-11-24T13:02:08.433728Z",
     "shell.execute_reply.started": "2024-11-24T13:02:08.376287Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46/46 [00:00<00:00, 5471.25 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 1345.43 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.35714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to Hugging Face dataset format\n",
    "train_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "test_dataset = Dataset.from_dict({\"text\": X_test, \"label\": y_test})\n",
    "\n",
    "# Map labels to IDs\n",
    "label2id = {\n",
    "    'stereotype': 0,\n",
    "    'unrelated': 1,\n",
    "    'neutral': 2,\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def map_labels(example):\n",
    "    example['label'] = label2id[example['label']]\n",
    "    return example\n",
    "\n",
    "# Apply the mapping to your dataset\n",
    "train_dataset = train_dataset.map(map_labels)\n",
    "test_dataset = test_dataset.map(map_labels)\n",
    "\n",
    "# Random Model Prediction\n",
    "random.seed(42)\n",
    "random_predictions = [random.choice(y_test) for _ in range(len(y_test))]\n",
    "\n",
    "# Evaluate the model\n",
    "f1 = f1_score(y_test, random_predictions, average='macro')\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T13:02:08.435933Z",
     "iopub.status.busy": "2024-11-24T13:02:08.435662Z",
     "iopub.status.idle": "2024-11-24T13:02:08.508814Z",
     "shell.execute_reply": "2024-11-24T13:02:08.507477Z",
     "shell.execute_reply.started": "2024-11-24T13:02:08.435907Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 17:28:45] Invalid gpu_ids format. Expected a string or a list of ints.\n",
      "[codecarbon INFO @ 17:28:45] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 17:28:45] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 17:28:45] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 17:28:45] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 17:28:45] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 17:28:47] We saw that you have a 13th Gen Intel(R) Core(TM) i7-13700H but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 17:28:47] CPU Model on constant consumption mode: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 17:28:47] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 17:28:47]   Platform system: Windows-10-10.0.22631-SP0\n",
      "[codecarbon INFO @ 17:28:47]   Python version: 3.10.15\n",
      "[codecarbon INFO @ 17:28:47]   CodeCarbon version: 2.4.2\n",
      "[codecarbon INFO @ 17:28:47]   Available RAM : 31.679 GB\n",
      "[codecarbon INFO @ 17:28:47]   CPU count: 20\n",
      "[codecarbon INFO @ 17:28:47]   CPU model: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 17:28:47]   GPU count: 1\n",
      "[codecarbon INFO @ 17:28:47]   GPU model: 1 x NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.17777777777777778\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'emissions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#emissions = tracker.stop()\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining carbon emissions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43memissions\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m kg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emissions' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "X_train = train_dataset['text']\n",
    "y_train = train_dataset['label']  \n",
    "X_test = test_dataset['text']\n",
    "y_test = test_dataset['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Logistic Regression Model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Tracking emissions with CodeCarbon\n",
    "tracker = EmissionsTracker()\n",
    "#tracker.start()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(X_test_tfidf)\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "#emissions = tracker.stop()\n",
    "print(f\"F1 Score: {f1}\")\n",
    "#print(f\"Training carbon emissions: {emissions} kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-24T13:02:08.509390Z",
     "iopub.status.idle": "2024-11-24T13:02:08.509676Z",
     "shell.execute_reply": "2024-11-24T13:02:08.509553Z",
     "shell.execute_reply.started": "2024-11-24T13:02:08.509538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True)\n",
    "    \n",
    "# Apply the tokenizer to the dataset\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-24T13:02:08.510804Z",
     "iopub.status.idle": "2024-11-24T13:02:08.511098Z",
     "shell.execute_reply": "2024-11-24T13:02:08.510972Z",
     "shell.execute_reply.started": "2024-11-24T13:02:08.510957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained ALBERT model with classification head\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#    \"albert-base-v2\", \n",
    "#    num_labels=3, \n",
    "#    label2id=label2id,\n",
    "#    id2label=id2label\n",
    "#)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=3,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # \"mps\" For macOS (Apple Silicon)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-24T13:02:08.512096Z",
     "iopub.status.idle": "2024-11-24T13:02:08.512441Z",
     "shell.execute_reply": "2024-11-24T13:02:08.512313Z",
     "shell.execute_reply.started": "2024-11-24T13:02:08.512297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fine-tuning the model and save the best model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    num_train_epochs=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,  # Use macro F1 computation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-24T13:02:08.515255Z",
     "iopub.status.idle": "2024-11-24T13:02:08.515695Z",
     "shell.execute_reply": "2024-11-24T13:02:08.515491Z",
     "shell.execute_reply.started": "2024-11-24T13:02:08.515469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tracking emissions with CodeCarbon\n",
    "tracker = EmissionsTracker()\n",
    "#tracker.start()\n",
    "\n",
    "#trainer.train()\n",
    "\n",
    "#emissions = tracker.stop()\n",
    "\n",
    "print()\n",
    "print(f\"Training carbon emissions: {emissions} kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-24T13:02:08.516645Z",
     "iopub.status.idle": "2024-11-24T13:02:08.517061Z",
     "shell.execute_reply": "2024-11-24T13:02:08.516866Z",
     "shell.execute_reply.started": "2024-11-24T13:02:08.516844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-24T13:02:08.519027Z",
     "iopub.status.idle": "2024-11-24T13:02:08.519675Z",
     "shell.execute_reply": "2024-11-24T13:02:08.519455Z",
     "shell.execute_reply.started": "2024-11-24T13:02:08.519432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "model_name = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels=3, label2id=label2id, id2label=id2label)\n",
    "\n",
    "import gc\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class MemoryCleanupCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# **1. Preprocess Text**\n",
    "def preprocess_text(examples):\n",
    "    text = examples['text']\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    examples['text'] = text\n",
    "    return examples\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_text)\n",
    "test_dataset = test_dataset.map(preprocess_text)\n",
    "\n",
    "# **2. Convert to Pandas DataFrame for Oversampling**\n",
    "train_df = train_dataset.to_pandas()\n",
    "\n",
    "# Separate features and labels\n",
    "X = train_df[['text']]\n",
    "y = train_df['label']\n",
    "\n",
    "# **3. Over-Sample Minority Classes**\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Convert back to Hugging Face Dataset\n",
    "resampled_df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "resampled_dataset = Dataset.from_pandas(resampled_df)\n",
    "\n",
    "# **4. Data Augmentation**\n",
    "def augment_text(examples):\n",
    "    aug = naw.SynonymAug()\n",
    "    augmented_texts = [\" \".join(text) if isinstance(text, list) else text for text in examples['text']]\n",
    "    return {\"text\": augmented_texts, \"label\": examples['label']}\n",
    "\n",
    "# Apply augmentation\n",
    "augmented_dataset = resampled_dataset.map(augment_text, batched=True)\n",
    "\n",
    "# Combine with resampled dataset\n",
    "combined_dataset = concatenate_datasets([resampled_dataset, augmented_dataset])\n",
    "\n",
    "# **5. Tokenization**\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=100  # Adjust to a lower value if possible\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_train_dataset = combined_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# **6. Update Labels and Compute Class Weights**\n",
    "train_labels = tokenized_train_dataset['label']\n",
    "\n",
    "# Check class distribution\n",
    "label_counts = Counter(train_labels)\n",
    "print(\"Class distribution after oversampling and augmentation:\", label_counts)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# **7. Determine Device**\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# **8. Load Config and Customize Model**\n",
    "label2id = {0: 'class_0', 1: 'class_1', 2: 'class_2'}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "#config = AutoConfig.from_pretrained(\n",
    "#    \"roberta-large\",\n",
    "#    num_labels=3,\n",
    "#    label2id=label2id,\n",
    "#    id2label=id2label\n",
    "#)\n",
    "\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class CustomRoberta(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        sequence_output = outputs[0]  # [batch_size, seq_length, hidden_size]\n",
    "        pooled_output = sequence_output[:, 0, :]  # [batch_size, hidden_size]\n",
    "        logits = self.classifier(pooled_output)  # [batch_size, num_labels]\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model = CustomRoberta.from_pretrained(\"roberta-large\", config=config)\n",
    "model = CustomRoberta.from_pretrained(model_name, config=config)\n",
    "model.to(device)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# **9. Define Training Arguments**\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# **10. Initialize Optimizer and Scheduler**\n",
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=training_args.weight_decay)\n",
    "\n",
    "total_steps = (\n",
    "    len(tokenized_train_dataset) // training_args.per_device_train_batch_size\n",
    ") * training_args.num_train_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=training_args.warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# **11. Define Custom Trainer Class**\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").to(device)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute custom loss with class weights on the correct device\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights.to(device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        if return_outputs:\n",
    "            return (loss, outputs)\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "        return loss\n",
    "\n",
    "# **12. Define Compute Metrics Function**\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(p.label_ids, preds),\n",
    "        'f1': f1_score(p.label_ids, preds, average='macro')\n",
    "    }\n",
    "\n",
    "# **13. Initialize the Custom Trainer**\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    callbacks=[MemoryCleanupCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-24T13:02:08.520621Z",
     "iopub.status.idle": "2024-11-24T13:02:08.521036Z",
     "shell.execute_reply": "2024-11-24T13:02:08.520841Z",
     "shell.execute_reply.started": "2024-11-24T13:02:08.520820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-24T13:02:08.522332Z",
     "iopub.status.idle": "2024-11-24T13:02:08.522629Z",
     "shell.execute_reply": "2024-11-24T13:02:08.522503Z",
     "shell.execute_reply.started": "2024-11-24T13:02:08.522488Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Making predictions on the test set\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(tokenized_test_dataset)\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(tokenized_test_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMacro F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Making predictions on the test set\n",
    "preds = trainer.predict(tokenized_test_dataset).predictions.argmax(-1)\n",
    "f1 = f1_score(tokenized_test_dataset['label'], preds, average='macro')\n",
    "print(f\"Macro F1 Score: {f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "hai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

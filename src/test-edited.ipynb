{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install codecarbon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:08:01.051961Z","iopub.execute_input":"2024-11-24T11:08:01.052265Z","iopub.status.idle":"2024-11-24T11:08:15.538832Z","shell.execute_reply.started":"2024-11-24T11:08:01.052237Z","shell.execute_reply":"2024-11-24T11:08:15.537900Z"}},"outputs":[{"name":"stdout","text":"Collecting codecarbon\n  Downloading codecarbon-2.7.4-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: arrow in /opt/conda/lib/python3.10/site-packages (from codecarbon) (1.3.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from codecarbon) (8.1.7)\nCollecting fief-client[cli] (from codecarbon)\n  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from codecarbon) (2.2.2)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from codecarbon) (0.20.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from codecarbon) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from codecarbon) (9.0.0)\nRequirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from codecarbon) (11.4.1)\nCollecting questionary (from codecarbon)\n  Downloading questionary-2.0.1-py3-none-any.whl.metadata (5.4 kB)\nCollecting rapidfuzz (from codecarbon)\n  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from codecarbon) (2.32.3)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from codecarbon) (13.7.1)\nRequirement already satisfied: typer in /opt/conda/lib/python3.10/site-packages (from codecarbon) (0.12.3)\nRequirement already satisfied: python-dateutil>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from arrow->codecarbon) (2.9.0.post0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow->codecarbon) (2.9.0.20240316)\nRequirement already satisfied: httpx<0.28.0,>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (0.27.0)\nCollecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\nCollecting yaspin (from fief-client[cli]->codecarbon)\n  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas->codecarbon) (1.26.4)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->codecarbon) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->codecarbon) (2024.1)\nCollecting prompt_toolkit<=3.0.36,>=2.0 (from questionary->codecarbon)\n  Downloading prompt_toolkit-3.0.36-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->codecarbon) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->codecarbon) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->codecarbon) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->codecarbon) (2024.8.30)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->codecarbon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->codecarbon) (2.18.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from typer->codecarbon) (4.12.2)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer->codecarbon) (1.5.4)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\nRequirement already satisfied: cryptography>=3.4 in /opt/conda/lib/python3.10/site-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (42.0.8)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt_toolkit<=3.0.36,>=2.0->questionary->codecarbon) (0.2.13)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\nCollecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.16.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.2.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\nDownloading codecarbon-2.7.4-py3-none-any.whl (504 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.2/504.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading questionary-2.0.1-py3-none-any.whl (34 kB)\nDownloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fief_client-0.20.0-py3-none-any.whl (20 kB)\nDownloading yaspin-3.1.0-py3-none-any.whl (18 kB)\nDownloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\nInstalling collected packages: termcolor, rapidfuzz, prompt_toolkit, yaspin, questionary, jwcrypto, fief-client, codecarbon\n  Attempting uninstall: termcolor\n    Found existing installation: termcolor 2.4.0\n    Uninstalling termcolor-2.4.0:\n      Successfully uninstalled termcolor-2.4.0\n  Attempting uninstall: prompt_toolkit\n    Found existing installation: prompt_toolkit 3.0.47\n    Uninstalling prompt_toolkit-3.0.47:\n      Successfully uninstalled prompt_toolkit-3.0.47\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nipython 8.21.0 requires prompt-toolkit<3.1.0,>=3.0.41, but you have prompt-toolkit 3.0.36 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed codecarbon-2.7.4 fief-client-0.20.0 jwcrypto-1.5.6 prompt_toolkit-3.0.36 questionary-2.0.1 rapidfuzz-3.10.1 termcolor-2.3.0 yaspin-3.1.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport random\nimport torch\nfrom datasets import Dataset\nfrom sklearn.metrics import f1_score\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    pipeline,\n)\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:08:15.541043Z","iopub.execute_input":"2024-11-24T11:08:15.541870Z","iopub.status.idle":"2024-11-24T11:08:46.525042Z","shell.execute_reply.started":"2024-11-24T11:08:15.541827Z","shell.execute_reply":"2024-11-24T11:08:46.524208Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load the dataset\nsplits = {'train': 'train.csv', 'test': 'test.csv'}\ntrain_data = pd.read_csv(\"hf://datasets/holistic-ai/EMGSD/\" + splits[\"train\"])\ntest_data = pd.read_csv(\"hf://datasets/holistic-ai/EMGSD/\" + splits[\"test\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:08:46.526158Z","iopub.execute_input":"2024-11-24T11:08:46.526754Z","iopub.status.idle":"2024-11-24T11:08:47.968923Z","shell.execute_reply.started":"2024-11-24T11:08:46.526724Z","shell.execute_reply":"2024-11-24T11:08:47.968255Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Use a subset of the data for faster training\nsample_ratio = 0.001\ntrain_data = train_data.sample(frac=sample_ratio, random_state=42)\ntest_data = test_data.sample(frac=sample_ratio, random_state=42)\n\n# Prepare train and test sets by using both training and testing data\nX_train, y_train = train_data[\"text\"].values.tolist(), train_data[\"category\"].values.tolist()\nX_test, y_test = test_data[\"text\"].values.tolist(), test_data[\"category\"].values.tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:08:47.971448Z","iopub.execute_input":"2024-11-24T11:08:47.972077Z","iopub.status.idle":"2024-11-24T11:08:47.993810Z","shell.execute_reply.started":"2024-11-24T11:08:47.972036Z","shell.execute_reply":"2024-11-24T11:08:47.993007Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Function to compute Macro F1 score\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    f1 = f1_score(labels, predictions, average='macro')  # Use macro F1\n    return {\"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:08:47.994691Z","iopub.execute_input":"2024-11-24T11:08:47.994910Z","iopub.status.idle":"2024-11-24T11:08:47.999249Z","shell.execute_reply.started":"2024-11-24T11:08:47.994888Z","shell.execute_reply":"2024-11-24T11:08:47.998378Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Convert to Hugging Face dataset format\ntrain_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\ntest_dataset = Dataset.from_dict({\"text\": X_test, \"label\": y_test})\n\n# Map labels to IDs\nlabel2id = {\n    'stereotype': 0,\n    'unrelated': 1,\n    'neutral': 2,\n}\n\nid2label = {v: k for k, v in label2id.items()}\n\ndef map_labels(example):\n    example['label'] = label2id[example['label']]\n    return example\n\n# Apply the mapping to your dataset\ntrain_dataset = train_dataset.map(map_labels)\ntest_dataset = test_dataset.map(map_labels)\n\n# Random Model Prediction\nrandom.seed(42)\nrandom_predictions = [random.choice(y_test) for _ in range(len(y_test))]\n\n# Evaluate the model\nf1 = f1_score(y_test, random_predictions, average='macro')\nprint(f\"F1 Score: {f1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:08:48.000253Z","iopub.execute_input":"2024-11-24T11:08:48.000552Z","iopub.status.idle":"2024-11-24T11:08:48.124305Z","shell.execute_reply.started":"2024-11-24T11:08:48.000517Z","shell.execute_reply":"2024-11-24T11:08:48.123303Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/46 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b031e282c4e04ae28f4902eee98abe4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"153461ee1ec04ce7b97459f5ff8997c6"}},"metadata":{}},{"name":"stdout","text":"F1 Score: 0.35714285714285715\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom codecarbon import EmissionsTracker\n\n# TF-IDF Vectorizer\nX_train = train_dataset['text']\ny_train = train_dataset['label']  \nX_test = test_dataset['text']\ny_test = test_dataset['label']\n\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Logistic Regression Model\nmodel = LogisticRegression()\n\n# Tracking emissions with CodeCarbon\ntracker = EmissionsTracker()\ntracker.start()\n\n# Fit the model\nmodel.fit(X_train_tfidf, y_train)\n\n# Evaluate the model\npredictions = model.predict(X_test_tfidf)\nf1 = f1_score(y_test, predictions, average='macro')\n\nemissions = tracker.stop()\nprint(f\"F1 Score: {f1}\")\nprint(f\"Training carbon emissions: {emissions} kg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:08:48.125309Z","iopub.execute_input":"2024-11-24T11:08:48.125558Z","iopub.status.idle":"2024-11-24T11:08:52.865971Z","shell.execute_reply.started":"2024-11-24T11:08:48.125534Z","shell.execute_reply":"2024-11-24T11:08:52.864884Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 11:08:48] [setup] RAM Tracking...\n[codecarbon INFO @ 11:08:48] [setup] GPU Tracking...\n[codecarbon INFO @ 11:08:48] Tracking Nvidia GPU via pynvml\n[codecarbon INFO @ 11:08:48] [setup] CPU Tracking...\n[codecarbon WARNING @ 11:08:48] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 11:08:49] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 11:08:49] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 11:08:49] >>> Tracker's metadata:\n[codecarbon INFO @ 11:08:49]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 11:08:49]   Python version: 3.10.14\n[codecarbon INFO @ 11:08:49]   CodeCarbon version: 2.7.4\n[codecarbon INFO @ 11:08:49]   Available RAM : 31.351 GB\n[codecarbon INFO @ 11:08:49]   CPU count: 4\n[codecarbon INFO @ 11:08:49]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 11:08:49]   GPU count: 2\n[codecarbon INFO @ 11:08:49]   GPU model: 2 x Tesla T4\n[codecarbon INFO @ 11:08:52] Saving emissions data to file /kaggle/working/emissions.csv\n[codecarbon INFO @ 11:08:52] Energy consumed for RAM : 0.000000 kWh. RAM Power : 11.756438255310059 W\n[codecarbon INFO @ 11:08:52] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 28.888357270690776 W\n[codecarbon INFO @ 11:08:52] Energy consumed for all CPUs : 0.000000 kWh. Total CPU Power : 42.5 W\n[codecarbon INFO @ 11:08:52] 0.000001 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.17777777777777778\nTraining carbon emissions: 3.78075521780836e-07 kg\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/codecarbon/output_methods/file.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nimport matplotlib.pyplot as plt\n\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n\n# Tokenization function\ndef tokenize_function(example):\n    return tokenizer(example['text'], padding='max_length', truncation=True)\n    \n# Apply the tokenizer to the dataset\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:08:52.867163Z","iopub.execute_input":"2024-11-24T11:08:52.867437Z","iopub.status.idle":"2024-11-24T11:08:54.023038Z","shell.execute_reply.started":"2024-11-24T11:08:52.867410Z","shell.execute_reply":"2024-11-24T11:08:54.022205Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f67e6c07d6a34a48bdcccc722df4beeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf2449589d144505a16be61c6dc7ff09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b3bd6edf6f24032810b7be12fb93c01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"795b458080784ada8700c416f15e5ace"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/46 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5c01e32953546e6972544431d8b2356"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3104274e1b1e49d29d45e237d6db6d1f"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Load pre-trained ALBERT model with classification head\n#model = AutoModelForSequenceClassification.from_pretrained(\n#    \"albert-base-v2\", \n#    num_labels=3, \n#    label2id=label2id,\n#    id2label=id2label\n#)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"roberta-base\",\n    num_labels=3,\n    label2id=label2id,\n    id2label=id2label\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # \"mps\" For macOS (Apple Silicon)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:08:54.024355Z","iopub.execute_input":"2024-11-24T11:08:54.025044Z","iopub.status.idle":"2024-11-24T11:08:57.232320Z","shell.execute_reply.started":"2024-11-24T11:08:54.025001Z","shell.execute_reply":"2024-11-24T11:08:57.231415Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25cf881d959848ab99bbc113ce6a5785"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b6e978be438419199bbd845f500b073"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Fine-tuning the model and save the best model\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    eval_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    logging_dir='./logs',\n    num_train_epochs=3,\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_total_limit=1,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True\n)\n\ntrainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    compute_metrics=compute_metrics,  # Use macro F1 computation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:08:57.236232Z","iopub.execute_input":"2024-11-24T11:08:57.236530Z","iopub.status.idle":"2024-11-24T11:09:03.052245Z","shell.execute_reply.started":"2024-11-24T11:08:57.236503Z","shell.execute_reply":"2024-11-24T11:09:03.051408Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 11:08:58] [setup] RAM Tracking...\n[codecarbon INFO @ 11:08:58] [setup] GPU Tracking...\n[codecarbon INFO @ 11:08:58] Tracking Nvidia GPU via pynvml\n[codecarbon INFO @ 11:08:58] [setup] CPU Tracking...\n[codecarbon WARNING @ 11:08:58] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 11:08:59] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 11:08:59] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 11:08:59] >>> Tracker's metadata:\n[codecarbon INFO @ 11:08:59]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 11:08:59]   Python version: 3.10.14\n[codecarbon INFO @ 11:08:59]   CodeCarbon version: 2.7.4\n[codecarbon INFO @ 11:08:59]   Available RAM : 31.351 GB\n[codecarbon INFO @ 11:08:59]   CPU count: 4\n[codecarbon INFO @ 11:08:59]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 11:08:59]   GPU count: 2\n[codecarbon INFO @ 11:08:59]   GPU model: 2 x Tesla T4\n[codecarbon INFO @ 11:09:03] Saving emissions data to file /kaggle/working/results/emissions.csv\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Tracking emissions with CodeCarbon\ntracker = EmissionsTracker()\ntracker.start()\n\n#trainer.train()\n\nemissions = tracker.stop()\n\nprint()\nprint(f\"Training carbon emissions: {emissions} kg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:09:03.053537Z","iopub.execute_input":"2024-11-24T11:09:03.053918Z","iopub.status.idle":"2024-11-24T11:09:03.063236Z","shell.execute_reply.started":"2024-11-24T11:09:03.053876Z","shell.execute_reply":"2024-11-24T11:09:03.062328Z"}},"outputs":[{"name":"stderr","text":"[codecarbon ERROR @ 11:09:03] Error: Another instance of codecarbon is already running. Turn off the other instance to be able to run this one. Exiting.\n[codecarbon WARNING @ 11:09:03] Another instance of codecarbon is already running. Exiting.\n[codecarbon WARNING @ 11:09:03] Another instance of codecarbon is already running. Exiting.\n","output_type":"stream"},{"name":"stdout","text":"\nTraining carbon emissions: None kg\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install nlpaug","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:09:50.604802Z","iopub.execute_input":"2024-11-24T11:09:50.605647Z","iopub.status.idle":"2024-11-24T11:09:59.833062Z","shell.execute_reply.started":"2024-11-24T11:09:50.605612Z","shell.execute_reply":"2024-11-24T11:09:59.832188Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (2.2.2)\nRequirement already satisfied: requests>=2.22.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (2.32.3)\nCollecting gdown>=4.0.0 (from nlpaug)\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (3.15.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.66.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (2024.8.30)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown, nlpaug\nSuccessfully installed gdown-5.2.0 nlpaug-1.1.11\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig,\n    Trainer,\n    TrainingArguments,\n    AdamW,\n    get_linear_schedule_with_warmup\n)\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.nn import CrossEntropyLoss\nfrom datasets import Dataset, concatenate_datasets\nimport pandas as pd\nimport re\nfrom collections import Counter\nfrom imblearn.over_sampling import RandomOverSampler\nimport nlpaug.augmenter.word as naw\n\n# **1. Preprocess Text**\ndef preprocess_text(examples):\n    text = examples['text']\n    # Convert to lowercase\n    text = text.lower()\n    # Remove URLs\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n    # Remove special characters\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    examples['text'] = text\n    return examples\n\n# Apply preprocessing\ntrain_dataset = train_dataset.map(preprocess_text)\ntest_dataset = test_dataset.map(preprocess_text)\n\n# **2. Convert to Pandas DataFrame for Oversampling**\ntrain_df = train_dataset.to_pandas()\n\n# Separate features and labels\nX = train_df[['text']]\ny = train_df['label']\n\n# **3. Over-Sample Minority Classes**\nros = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = ros.fit_resample(X, y)\n\n# Convert back to Hugging Face Dataset\nresampled_df = pd.concat([X_resampled, y_resampled], axis=1)\nresampled_dataset = Dataset.from_pandas(resampled_df)\n\n# **4. Data Augmentation**\ndef augment_text(examples):\n    aug = naw.SynonymAug()\n    augmented_texts = [\" \".join(text) if isinstance(text, list) else text for text in examples['text']]\n    return {\"text\": augmented_texts, \"label\": examples['label']}\n\n# Apply augmentation\naugmented_dataset = resampled_dataset.map(augment_text, batched=True)\n\n# Combine with resampled dataset\ncombined_dataset = concatenate_datasets([resampled_dataset, augmented_dataset])\n\n# **5. Tokenization**\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True)\n\ntokenized_train_dataset = combined_dataset.map(tokenize_function, batched=True)\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# **6. Update Labels and Compute Class Weights**\ntrain_labels = tokenized_train_dataset['label']\n\n# Check class distribution\nlabel_counts = Counter(train_labels)\nprint(\"Class distribution after oversampling and augmentation:\", label_counts)\n\n# Compute class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = class_weights / class_weights.sum()\nclass_weights = torch.tensor(class_weights, dtype=torch.float)\n\n# **7. Determine Device**\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# **8. Load Config and Customize Model**\nlabel2id = {0: 'class_0', 1: 'class_1', 2: 'class_2'}\nid2label = {v: k for k, v in label2id.items()}\n\nconfig = AutoConfig.from_pretrained(\n    \"roberta-large\",\n    num_labels=3,\n    label2id=label2id,\n    id2label=id2label\n)\n\nfrom transformers import RobertaForSequenceClassification\n\nclass CustomRoberta(RobertaForSequenceClassification):\n    def __init__(self, config):\n        super().__init__(config)\n        # Customize the classifier\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(config.hidden_size, 512),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=0.1),\n            torch.nn.Linear(512, config.num_labels)\n        )\n\nmodel = CustomRoberta.from_pretrained(\"roberta-large\", config=config)\nmodel.to(device)\nclass_weights = class_weights.to(device)\n\n# **9. Define Training Arguments**\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    learning_rate=1e-5,\n    weight_decay=0.01,\n    warmup_steps=100,\n    logging_dir='./logs',\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_total_limit=1,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    fp16=torch.cuda.is_available(),\n)\n\n# **10. Initialize Optimizer and Scheduler**\noptimizer = AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=training_args.weight_decay)\n\ntotal_steps = (\n    len(tokenized_train_dataset) // training_args.per_device_train_batch_size\n) * training_args.num_train_epochs\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=training_args.warmup_steps,\n    num_training_steps=total_steps\n)\n\n# **11. Define Custom Trainer Class**\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\").to(device)\n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # Reshape logits to match the shape of labels\n        logits = logits.view(-1, model.config.num_labels)\n        labels = labels.view(-1)\n        # Compute custom loss with class weights\n        loss_fct = CrossEntropyLoss(weight=class_weights)\n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n    def training_step(self, model, inputs, num_items_in_batch=None):\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n        loss = self.compute_loss(model, inputs)\n        return loss\n\n# **12. Define Compute Metrics Function**\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\n        'accuracy': accuracy_score(p.label_ids, preds),\n        'f1': f1_score(p.label_ids, preds, average='macro')\n    }\n\n# **13. Initialize the Custom Trainer**\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    optimizers=(optimizer, scheduler)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:10:05.493519Z","iopub.execute_input":"2024-11-24T11:10:05.494200Z","iopub.status.idle":"2024-11-24T11:10:38.405721Z","shell.execute_reply.started":"2024-11-24T11:10:05.494165Z","shell.execute_reply":"2024-11-24T11:10:38.404763Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/46 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7739ce27d0e483daefe466c038c7bba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc7f4b478cbf45679051284a61ae46c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/54 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99c6845849464544874c9a6f61fbfae3"}},"metadata":{}},{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f945d5ef4334e76a6e04a6d16c81248"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaf5bb31f57d4a9aa937b0f1005fd5c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c07bb00798448d9892416df11268f11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cab4a43739d414dbe84395d460694bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"928b17c4dcc34e308598105d1896abe6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/108 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"782f7e7400f14b8fbaed675bd6a01a0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79da344ac58b45a791152674a058507e"}},"metadata":{}},{"name":"stdout","text":"Class distribution after oversampling and augmentation: Counter({2: 36, 1: 36, 0: 36})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffd9cea1a89140328562f5b71fbf5c41"}},"metadata":{}},{"name":"stderr","text":"Some weights of CustomRoberta were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.3.bias', 'classifier.3.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n[codecarbon ERROR @ 11:10:38] Error: Another instance of codecarbon is already running. Turn off the other instance to be able to run this one. Exiting.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Start training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:10:43.870989Z","iopub.execute_input":"2024-11-24T11:10:43.871651Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n# Making predictions on the test set\npreds = trainer.predict(tokenized_test_dataset).predictions.argmax(-1)\nf1 = f1_score(tokenized_test_dataset['label'], preds, average='macro')\nprint(f\"Macro F1 Score: {f1:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:09:03.859499Z","iopub.status.idle":"2024-11-24T11:09:03.859846Z","shell.execute_reply.started":"2024-11-24T11:09:03.859696Z","shell.execute_reply":"2024-11-24T11:09:03.859712Z"}},"outputs":[],"execution_count":null}]}
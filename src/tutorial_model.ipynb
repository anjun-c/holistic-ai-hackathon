{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stereotype_type</th>\n",
       "      <th>text</th>\n",
       "      <th>text_with_marker</th>\n",
       "      <th>category</th>\n",
       "      <th>data_source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>profession</td>\n",
       "      <td>The illustrator had tower hands.</td>\n",
       "      <td>The illustrator had ===tower=== hands.</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>stereoset_intrasentence</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>profession</td>\n",
       "      <td>I gave that physician 5 stars because he was s...</td>\n",
       "      <td>I gave that physician 5 stars because he was s...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>stereoset_intrasentence</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nationality</td>\n",
       "      <td>Peru is a great place for a vacation the sky i...</td>\n",
       "      <td>Peru is a great place for a vacation ===the sk...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>stereoset_intersentence</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>profession</td>\n",
       "      <td>That chef cooked a great meal. Throw that fur ...</td>\n",
       "      <td>That chef cooked a great meal. ===Throw that f...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>stereoset_intersentence</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationality</td>\n",
       "      <td>Ecuadorian economy produces many toaster indiv...</td>\n",
       "      <td>Ecuadorian economy produces many ===toaster===...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>stereoset_intrasentence</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45755</th>\n",
       "      <td>lgbtq+</td>\n",
       "      <td>all Transgender people are associated with men...</td>\n",
       "      <td>all Transgender people are associated with men...</td>\n",
       "      <td>stereotype</td>\n",
       "      <td>winoqueer_augmented</td>\n",
       "      <td>stereotype_lgbtq+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45756</th>\n",
       "      <td>profession</td>\n",
       "      <td>A prosecutor has a high attention to detail. T...</td>\n",
       "      <td>A prosecutor has a high attention to detail. =...</td>\n",
       "      <td>stereotype</td>\n",
       "      <td>stereoset_intersentence</td>\n",
       "      <td>stereotype_profession</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45757</th>\n",
       "      <td>nationality</td>\n",
       "      <td>The people from singapore are normally smart</td>\n",
       "      <td>The people from singapore are normally ===smar...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>stereoset_intrasentence</td>\n",
       "      <td>neutral_nationality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45758</th>\n",
       "      <td>nationality</td>\n",
       "      <td>The guatemalan wealthy man was having a rough ...</td>\n",
       "      <td>The guatemalan ===wealthy=== man was having a ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>stereoset_intrasentence</td>\n",
       "      <td>neutral_nationality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45759</th>\n",
       "      <td>lgbtq+</td>\n",
       "      <td>all Bisexual people are mirrors.</td>\n",
       "      <td>all Bisexual people are mirrors.</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>winoqueer_augmented</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45760 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      stereotype_type                                               text  \\\n",
       "0          profession                   The illustrator had tower hands.   \n",
       "1          profession  I gave that physician 5 stars because he was s...   \n",
       "2         nationality  Peru is a great place for a vacation the sky i...   \n",
       "3          profession  That chef cooked a great meal. Throw that fur ...   \n",
       "4         nationality  Ecuadorian economy produces many toaster indiv...   \n",
       "...               ...                                                ...   \n",
       "45755          lgbtq+  all Transgender people are associated with men...   \n",
       "45756      profession  A prosecutor has a high attention to detail. T...   \n",
       "45757     nationality       The people from singapore are normally smart   \n",
       "45758     nationality  The guatemalan wealthy man was having a rough ...   \n",
       "45759          lgbtq+                   all Bisexual people are mirrors.   \n",
       "\n",
       "                                        text_with_marker    category  \\\n",
       "0                 The illustrator had ===tower=== hands.   unrelated   \n",
       "1      I gave that physician 5 stars because he was s...   unrelated   \n",
       "2      Peru is a great place for a vacation ===the sk...   unrelated   \n",
       "3      That chef cooked a great meal. ===Throw that f...   unrelated   \n",
       "4      Ecuadorian economy produces many ===toaster===...   unrelated   \n",
       "...                                                  ...         ...   \n",
       "45755  all Transgender people are associated with men...  stereotype   \n",
       "45756  A prosecutor has a high attention to detail. =...  stereotype   \n",
       "45757  The people from singapore are normally ===smar...     neutral   \n",
       "45758  The guatemalan ===wealthy=== man was having a ...     neutral   \n",
       "45759                   all Bisexual people are mirrors.   unrelated   \n",
       "\n",
       "                   data_source                  label  \n",
       "0      stereoset_intrasentence              unrelated  \n",
       "1      stereoset_intrasentence              unrelated  \n",
       "2      stereoset_intersentence              unrelated  \n",
       "3      stereoset_intersentence              unrelated  \n",
       "4      stereoset_intrasentence              unrelated  \n",
       "...                        ...                    ...  \n",
       "45755      winoqueer_augmented      stereotype_lgbtq+  \n",
       "45756  stereoset_intersentence  stereotype_profession  \n",
       "45757  stereoset_intrasentence    neutral_nationality  \n",
       "45758  stereoset_intrasentence    neutral_nationality  \n",
       "45759      winoqueer_augmented              unrelated  \n",
       "\n",
       "[45760 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "train_data = pd.read_csv(\"hf://datasets/holistic-ai/EMGSD/\" + splits[\"train\"])\n",
    "test_data = pd.read_csv(\"hf://datasets/holistic-ai/EMGSD/\" + splits[\"test\"])\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stereotype_type</th>\n",
       "      <th>text</th>\n",
       "      <th>text_with_marker</th>\n",
       "      <th>category</th>\n",
       "      <th>data_source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3705</th>\n",
       "      <td>nationality</td>\n",
       "      <td>Would most americans treat paraguayan culture ...</td>\n",
       "      <td>Would most americans treat paraguayan culture ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>stereoset_intrasentence</td>\n",
       "      <td>neutral_nationality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45701</th>\n",
       "      <td>nationality</td>\n",
       "      <td>swedish people are in europe white is the colo...</td>\n",
       "      <td>swedish people are in europe ===white is the c...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>stereoset_intersentence</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45647</th>\n",
       "      <td>nationality</td>\n",
       "      <td>Bangladesh's environment towards women is oppr...</td>\n",
       "      <td>Bangladesh's environment towards women is ===o...</td>\n",
       "      <td>stereotype</td>\n",
       "      <td>stereoset_intrasentence</td>\n",
       "      <td>stereotype_nationality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4962</th>\n",
       "      <td>lgbtq+</td>\n",
       "      <td>most people who are Gay have varied expression...</td>\n",
       "      <td>most people who are Gay have varied expression...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>winoqueer_augmented</td>\n",
       "      <td>neutral_lgbtq+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16441</th>\n",
       "      <td>nationality</td>\n",
       "      <td>The neighborhood's view of the new eritrean fa...</td>\n",
       "      <td>The neighborhood's view of the new eritrean fa...</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>stereoset_intrasentence</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      stereotype_type                                               text  \\\n",
       "3705      nationality  Would most americans treat paraguayan culture ...   \n",
       "45701     nationality  swedish people are in europe white is the colo...   \n",
       "45647     nationality  Bangladesh's environment towards women is oppr...   \n",
       "4962           lgbtq+  most people who are Gay have varied expression...   \n",
       "16441     nationality  The neighborhood's view of the new eritrean fa...   \n",
       "\n",
       "                                        text_with_marker    category  \\\n",
       "3705   Would most americans treat paraguayan culture ...     neutral   \n",
       "45701  swedish people are in europe ===white is the c...   unrelated   \n",
       "45647  Bangladesh's environment towards women is ===o...  stereotype   \n",
       "4962   most people who are Gay have varied expression...     neutral   \n",
       "16441  The neighborhood's view of the new eritrean fa...   unrelated   \n",
       "\n",
       "                   data_source                   label  \n",
       "3705   stereoset_intrasentence     neutral_nationality  \n",
       "45701  stereoset_intersentence               unrelated  \n",
       "45647  stereoset_intrasentence  stereotype_nationality  \n",
       "4962       winoqueer_augmented          neutral_lgbtq+  \n",
       "16441  stereoset_intrasentence               unrelated  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a subset of the data for faster training\n",
    "sample_ratio = 0.001\n",
    "train_data = train_data.sample(frac=sample_ratio, random_state=42)\n",
    "test_data = test_data.sample(frac=sample_ratio, random_state=42)\n",
    "\n",
    "# Prepare train and test sets by using both training and testing data\n",
    "X_train, y_train = train_data[\"text\"].values.tolist(), train_data[\"category\"].values.tolist()\n",
    "X_test, y_test = test_data[\"text\"].values.tolist(), test_data[\"category\"].values.tolist()\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute Macro F1 score\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_score(labels, predictions, average='macro')  # Use macro F1\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46/46 [00:00<00:00, 7686.47 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 5721.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.35714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "train_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "test_dataset = Dataset.from_dict({\"text\": X_test, \"label\": y_test})\n",
    "\n",
    "# Map labels to IDs\n",
    "label2id = {\n",
    "    'stereotype': 0,\n",
    "    'unrelated': 1,\n",
    "    'neutral': 2,\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def map_labels(example):\n",
    "    example['label'] = label2id[example['label']]\n",
    "    return example\n",
    "\n",
    "# Apply the mapping to your dataset\n",
    "train_dataset = train_dataset.map(map_labels)\n",
    "test_dataset = test_dataset.map(map_labels)\n",
    "\n",
    "# Random Model Prediction\n",
    "random.seed(42)\n",
    "random_predictions = [random.choice(y_test) for _ in range(len(y_test))]\n",
    "\n",
    "# Evaluate the model\n",
    "f1 = f1_score(y_test, random_predictions, average='macro')\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmissionsTracker\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# TF-IDF Vectorizer\u001b[39;00m\n\u001b[0;32m      6\u001b[0m X_train \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\codecarbon\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe Carbon Tracker module. The following objects/decorators belong to the Public API\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01memissions_tracker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     EmissionsTracker,\n\u001b[0;32m      8\u001b[0m     OfflineEmissionsTracker,\n\u001b[0;32m      9\u001b[0m     track_emissions,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmissionsTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOfflineEmissionsTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_emissions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\codecarbon\\emissions_tracker.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, List, Optional, Union\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cpu, gpu, powermetrics\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_hierarchical_config, parse_gpu_ids\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01memissions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Emissions\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\codecarbon\\core\\cpu.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect_cpu_model\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataSource\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_powergadget_available\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\codecarbon\\input.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataSource\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\pkg_resources\\__init__.py:95\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjaraco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drop_comment, join_continuation, yield_lines\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplatformdirs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m user_cache_dir \u001b[38;5;28;01mas\u001b[39;00m _user_cache_dir\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1548\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1591\u001b[0m, in \u001b[0;36m_fill_cache\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "X_train = train_dataset['text']\n",
    "y_train = train_dataset['label']  \n",
    "X_test = test_dataset['text']\n",
    "y_test = test_dataset['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Logistic Regression Model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Tracking emissions with CodeCarbon\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(X_test_tfidf)\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "emissions = tracker.stop()\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Training carbon emissions: {emissions} kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46/46 [00:00<00:00, 1082.67 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 716.94 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "New column name labels already in the dataset. Please choose a column name which is not already in the dataset. Current columns in the dataset: ['label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m tokenized_test_dataset \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_train_dataset\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m---> 22\u001b[0m tokenized_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_train_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m tokenized_test_dataset \u001b[38;5;241m=\u001b[39m tokenized_test_dataset\u001b[38;5;241m.\u001b[39mrename_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Load pre-trained ALBERT model with classification head\u001b[39;00m\n",
      "File \u001b[1;32mc:\\USERS\\ADRIA\\ANACONDA3\\ENVS\\HOLISTIC_AI_ENV\\lib\\site-packages\\datasets\\fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m func(dataset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\USERS\\ADRIA\\ANACONDA3\\ENVS\\HOLISTIC_AI_ENV\\lib\\site-packages\\datasets\\arrow_dataset.py:2198\u001b[0m, in \u001b[0;36mDataset.rename_column\u001b[1;34m(self, original_column_name, new_column_name, new_fingerprint)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2196\u001b[0m     )\n\u001b[0;32m   2197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_column_name \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[1;32m-> 2198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease choose a column name which is not already in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2202\u001b[0m     )\n\u001b[0;32m   2203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_column_name:\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew column name is empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: New column name labels already in the dataset. Please choose a column name which is not already in the dataset. Current columns in the dataset: ['label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(example):\n",
    "    tokenized_inputs = tokenizer(example['text'], padding='max_length', truncation=True)\n",
    "    tokenized_inputs['labels'] = example['label']\n",
    "    return tokenized_inputs\n",
    "\n",
    "    \n",
    "# Apply the tokenizer to the dataset\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_train_dataset.column_names)\n",
    "\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "\n",
    "\n",
    "# Load pre-trained ALBERT model with classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"albert-base-v2\", \n",
    "    num_labels=3, \n",
    "    label2id=label2id,\n",
    "    id2label=id2label\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # \"mps\" For macOS (Apple Silicon)\n",
    "model.to(device)\n",
    "\n",
    "# Tracking emissions with CodeCarbon\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "# Fine-tuning the model and save the best model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    num_train_epochs=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,  # Use macro F1 computation\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "emissions = tracker.stop()\n",
    "print()\n",
    "print(f\"Training carbon emissions: {emissions} kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Making predictions on the test set\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(tokenized_test_dataset)\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(tokenized_test_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMacro F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Making predictions on the test set\n",
    "preds = trainer.predict(tokenized_test_dataset).predictions.argmax(-1)\n",
    "f1 = f1_score(tokenized_test_dataset['label'], preds, average='macro')\n",
    "print(f\"Macro F1 Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the text classification pipeline\n",
    "classifier = pipeline(\"text-classification\", model=trainer.model, tokenizer=trainer.tokenizer,top_k=3)\n",
    "\n",
    "def assess_bias(model, counterfactuals):\n",
    "    bias_results = {}\n",
    "    for text in counterfactuals:\n",
    "        prediction = model(text)\n",
    "        bias_results[text] = prediction\n",
    "    return bias_results\n",
    "\n",
    "\n",
    "# Creating demographic counterfactuals (example)\n",
    "counterfactuals = [\"Males usually think that the chef is talented.\", \"Females often assume that the nurse is dedicated.\"]\n",
    "bias_results = assess_bias(classifier, counterfactuals)\n",
    "\n",
    "bias_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(classifier)\n",
    "shap_values = explainer(counterfactuals)\n",
    "\n",
    "shap_values_stereotype = shap_values[:, :, \"stereotype\"].values\n",
    "shap_vectors = []\n",
    "\n",
    "# Save SHAP values in vectors for subsequent calculation\n",
    "for index, values in enumerate(shap_values_stereotype):\n",
    "    # Trim to exclude whitespace and punctuation \n",
    "    trimmed_values = values[1:-2]\n",
    "    shap_vectors.append(trimmed_values)\n",
    "    print(f\"Sentence {index+1} SHAP vector: {trimmed_values}\")\n",
    "\n",
    "shap.plots.text(shap_values[:, :, \"stereotype\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "def predict_proba(texts):\n",
    "    preds = classifier(texts)\n",
    "    probabilities = np.array([[pred['score'] for pred in preds_single] for preds_single in preds])\n",
    "    return probabilities\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=[\"stereotype\", \"neutral\", \"unrelated\"])\n",
    "\n",
    "lime_values_per_sentence = []\n",
    "\n",
    "for idx, sentence in enumerate(counterfactuals):\n",
    "    exp = explainer.explain_instance(sentence, predict_proba, num_features=50, num_samples=100, top_labels=1)\n",
    "    feature_importances = exp.as_list(label=0)\n",
    "    \n",
    "    lime_values = [weight for _, weight in feature_importances]\n",
    "    lime_values_per_sentence.append(lime_values)\n",
    "    \n",
    "    print(f\"LIME values for Sentence {idx+1} 'stereotype':\", lime_values)\n",
    "\n",
    "    exp.show_in_notebook() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot and compare SHAP and LIME explanations\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(shap_vectors[0], label=\"SHAP\")\n",
    "plt.plot(lime_values_per_sentence[0], label=\"LIME\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Token position\")\n",
    "plt.ylabel(\"Explanation value\")\n",
    "plt.title(\"SHAP and LIME Explanations Comparison\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculating cosine similarity between SHAP and LIME vectors\n",
    "for idx, (shap_vec, lime_vec) in enumerate(zip(shap_vectors, lime_values_per_sentence)):\n",
    "    shap_vec_array = np.array(shap_vec)\n",
    "    lime_vec_array = np.array(lime_vec)\n",
    "\n",
    "    similarity = cosine_similarity([shap_vec_array], [lime_vec_array])[0][0]\n",
    "    print(f\"Cosine similarity between SHAP and LIME for Sentence {idx + 1} ({counterfactuals[idx]}): {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

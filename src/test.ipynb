{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "train_data = pd.read_csv(\"hf://datasets/holistic-ai/EMGSD/\" + splits[\"train\"])\n",
    "test_data = pd.read_csv(\"hf://datasets/holistic-ai/EMGSD/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset of the data for faster training\n",
    "sample_ratio = 0.001\n",
    "train_data = train_data.sample(frac=sample_ratio, random_state=42)\n",
    "test_data = test_data.sample(frac=sample_ratio, random_state=42)\n",
    "\n",
    "# Prepare train and test sets by using both training and testing data\n",
    "X_train, y_train = train_data[\"text\"].values.tolist(), train_data[\"category\"].values.tolist()\n",
    "X_test, y_test = test_data[\"text\"].values.tolist(), test_data[\"category\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Macro F1 score\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_score(labels, predictions, average='macro')  # Use macro F1\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46/46 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.35714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to Hugging Face dataset format\n",
    "train_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "test_dataset = Dataset.from_dict({\"text\": X_test, \"label\": y_test})\n",
    "\n",
    "# Map labels to IDs\n",
    "label2id = {\n",
    "    'stereotype': 0,\n",
    "    'unrelated': 1,\n",
    "    'neutral': 2,\n",
    "}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def map_labels(example):\n",
    "    example['label'] = label2id[example['label']]\n",
    "    return example\n",
    "\n",
    "# Apply the mapping to your dataset\n",
    "train_dataset = train_dataset.map(map_labels)\n",
    "test_dataset = test_dataset.map(map_labels)\n",
    "\n",
    "# Random Model Prediction\n",
    "random.seed(42)\n",
    "random_predictions = [random.choice(y_test) for _ in range(len(y_test))]\n",
    "\n",
    "# Evaluate the model\n",
    "f1 = f1_score(y_test, random_predictions, average='macro')\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 16:47:19] Invalid gpu_ids format. Expected a string or a list of ints.\n",
      "[codecarbon INFO @ 16:47:19] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 16:47:19] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 16:47:19] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 16:47:19] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 16:47:19] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 16:47:21] We saw that you have a 13th Gen Intel(R) Core(TM) i7-13700H but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 16:47:21] CPU Model on constant consumption mode: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 16:47:21] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 16:47:21]   Platform system: Windows-10-10.0.22631-SP0\n",
      "[codecarbon INFO @ 16:47:21]   Python version: 3.10.15\n",
      "[codecarbon INFO @ 16:47:21]   CodeCarbon version: 2.4.2\n",
      "[codecarbon INFO @ 16:47:21]   Available RAM : 31.679 GB\n",
      "[codecarbon INFO @ 16:47:21]   CPU count: 20\n",
      "[codecarbon INFO @ 16:47:21]   CPU model: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 16:47:21]   GPU count: 1\n",
      "[codecarbon INFO @ 16:47:21]   GPU model: 1 x NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "[codecarbon INFO @ 16:47:24] Energy consumed for RAM : 0.000000 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 16:47:24] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 16:47:24] Energy consumed for all CPUs : 0.000000 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 16:47:24] 0.000000 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.17777777777777778\n",
      "Training carbon emissions: 5.0235711066259276e-08 kg\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "X_train = train_dataset['text']\n",
    "y_train = train_dataset['label']  \n",
    "X_test = test_dataset['text']\n",
    "y_test = test_dataset['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Logistic Regression Model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Tracking emissions with CodeCarbon\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(X_test_tfidf)\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "emissions = tracker.stop()\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Training carbon emissions: {emissions} kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46/46 [00:00<00:00, 2812.02 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 692.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True)\n",
    "    \n",
    "# Apply the tokenizer to the dataset\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained ALBERT model with classification head\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#    \"albert-base-v2\", \n",
    "#    num_labels=3, \n",
    "#    label2id=label2id,\n",
    "#    id2label=id2label\n",
    "#)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=3,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # \"mps\" For macOS (Apple Silicon)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_43284\\9441889.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[codecarbon WARNING @ 16:47:25] Invalid gpu_ids format. Expected a string or a list of ints.\n",
      "[codecarbon INFO @ 16:47:25] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 16:47:25] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 16:47:25] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 16:47:25] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 16:47:25] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 16:47:26] We saw that you have a 13th Gen Intel(R) Core(TM) i7-13700H but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 16:47:26] CPU Model on constant consumption mode: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 16:47:26] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 16:47:26]   Platform system: Windows-10-10.0.22631-SP0\n",
      "[codecarbon INFO @ 16:47:26]   Python version: 3.10.15\n",
      "[codecarbon INFO @ 16:47:26]   CodeCarbon version: 2.4.2\n",
      "[codecarbon INFO @ 16:47:26]   Available RAM : 31.679 GB\n",
      "[codecarbon INFO @ 16:47:26]   CPU count: 20\n",
      "[codecarbon INFO @ 16:47:26]   CPU model: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 16:47:26]   GPU count: 1\n",
      "[codecarbon INFO @ 16:47:26]   GPU model: 1 x NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the model and save the best model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    num_train_epochs=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,  # Use macro F1 computation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 16:47:30] Invalid gpu_ids format. Expected a string or a list of ints.\n",
      "[codecarbon INFO @ 16:47:30] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 16:47:30] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 16:47:30] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 16:47:30] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 16:47:30] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 16:47:31] We saw that you have a 13th Gen Intel(R) Core(TM) i7-13700H but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 16:47:31] CPU Model on constant consumption mode: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 16:47:31] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 16:47:31]   Platform system: Windows-10-10.0.22631-SP0\n",
      "[codecarbon INFO @ 16:47:31]   Python version: 3.10.15\n",
      "[codecarbon INFO @ 16:47:31]   CodeCarbon version: 2.4.2\n",
      "[codecarbon INFO @ 16:47:31]   Available RAM : 31.679 GB\n",
      "[codecarbon INFO @ 16:47:31]   CPU count: 20\n",
      "[codecarbon INFO @ 16:47:31]   CPU model: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 16:47:31]   GPU count: 1\n",
      "[codecarbon INFO @ 16:47:31]   GPU model: 1 x NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "[codecarbon INFO @ 16:47:34] Energy consumed for RAM : 0.000000 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 16:47:34] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n",
      "[codecarbon INFO @ 16:47:34] Energy consumed for all CPUs : 0.000000 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 16:47:34] 0.000000 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training carbon emissions: 2.751324741017056e-08 kg\n"
     ]
    }
   ],
   "source": [
    "# Tracking emissions with CodeCarbon\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "#trainer.train()\n",
    "\n",
    "emissions = tracker.stop()\n",
    "\n",
    "print()\n",
    "print(f\"Training carbon emissions: {emissions} kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46/46 [00:00<00:00, 19253.37 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<00:00, 1067.23 examples/s]\n",
      "Map: 100%|██████████| 54/54 [00:02<00:00, 22.87 examples/s]\n",
      "Map: 100%|██████████| 108/108 [00:00<00:00, 5777.65 examples/s]\n",
      "Map: 100%|██████████| 11/11 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling and augmentation: Counter({2: 36, 1: 36, 0: 36})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomRoberta were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_43284\\2022825975.py:184: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "[codecarbon WARNING @ 16:47:37] Invalid gpu_ids format. Expected a string or a list of ints.\n",
      "[codecarbon INFO @ 16:47:37] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 16:47:37] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 16:47:37] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 16:47:37] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 16:47:37] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 16:47:39] We saw that you have a 13th Gen Intel(R) Core(TM) i7-13700H but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 16:47:39] CPU Model on constant consumption mode: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 16:47:39] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 16:47:39]   Platform system: Windows-10-10.0.22631-SP0\n",
      "[codecarbon INFO @ 16:47:39]   Python version: 3.10.15\n",
      "[codecarbon INFO @ 16:47:39]   CodeCarbon version: 2.4.2\n",
      "[codecarbon INFO @ 16:47:39]   Available RAM : 31.679 GB\n",
      "[codecarbon INFO @ 16:47:39]   CPU count: 20\n",
      "[codecarbon INFO @ 16:47:39]   CPU model: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 16:47:39]   GPU count: 1\n",
      "[codecarbon INFO @ 16:47:39]   GPU model: 1 x NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# **1. Preprocess Text**\n",
    "def preprocess_text(examples):\n",
    "    text = examples['text']\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    examples['text'] = text\n",
    "    return examples\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_text)\n",
    "test_dataset = test_dataset.map(preprocess_text)\n",
    "\n",
    "# **2. Convert to Pandas DataFrame for Oversampling**\n",
    "train_df = train_dataset.to_pandas()\n",
    "\n",
    "# Separate features and labels\n",
    "X = train_df[['text']]\n",
    "y = train_df['label']\n",
    "\n",
    "# **3. Over-Sample Minority Classes**\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Convert back to Hugging Face Dataset\n",
    "resampled_df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "resampled_dataset = Dataset.from_pandas(resampled_df)\n",
    "\n",
    "# **4. Data Augmentation**\n",
    "def augment_text(examples):\n",
    "    aug = naw.SynonymAug()\n",
    "    augmented_texts = [\" \".join(text) if isinstance(text, list) else text for text in examples['text']]\n",
    "    return {\"text\": augmented_texts, \"label\": examples['label']}\n",
    "\n",
    "# Apply augmentation\n",
    "augmented_dataset = resampled_dataset.map(augment_text, batched=True)\n",
    "\n",
    "# Combine with resampled dataset\n",
    "combined_dataset = concatenate_datasets([resampled_dataset, augmented_dataset])\n",
    "\n",
    "# **5. Tokenization**\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_train_dataset = combined_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# **6. Update Labels and Compute Class Weights**\n",
    "train_labels = tokenized_train_dataset['label']\n",
    "\n",
    "# Check class distribution\n",
    "label_counts = Counter(train_labels)\n",
    "print(\"Class distribution after oversampling and augmentation:\", label_counts)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# **7. Determine Device**\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# **8. Load Config and Customize Model**\n",
    "label2id = {0: 'class_0', 1: 'class_1', 2: 'class_2'}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"roberta-large\",\n",
    "    num_labels=3,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label\n",
    ")\n",
    "\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "class CustomRoberta(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Customize the classifier\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(config.hidden_size, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(512, config.num_labels)\n",
    "        )\n",
    "\n",
    "model = CustomRoberta.from_pretrained(\"roberta-large\", config=config)\n",
    "model.to(device)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# **9. Define Training Arguments**\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# **10. Initialize Optimizer and Scheduler**\n",
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=training_args.weight_decay)\n",
    "\n",
    "total_steps = (\n",
    "    len(tokenized_train_dataset) // training_args.per_device_train_batch_size\n",
    ") * training_args.num_train_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=training_args.warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# **11. Define Custom Trainer Class**\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\").to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Reshape logits to match the shape of labels\n",
    "        logits = logits.view(-1, model.config.num_labels)\n",
    "        labels = labels.view(-1)\n",
    "        # Compute custom loss with class weights\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "        return loss\n",
    "\n",
    "# **12. Define Compute Metrics Function**\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(p.label_ids, preds),\n",
    "        'f1': f1_score(p.label_ids, preds, average='macro')\n",
    "    }\n",
    "\n",
    "# **13. Initialize the Custom Trainer**\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (4096) to match target batch_size (8).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "Cell \u001b[1;32mIn[12], line 172\u001b[0m, in \u001b[0;36mCustomTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m    170\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    171\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_inputs(inputs)\n\u001b[1;32m--> 172\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[1;32mIn[12], line 159\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m    157\u001b[0m labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    160\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Reshape logits to match the shape of labels\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1352\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1351\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m-> 1352\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1354\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\hai_env\\lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (4096) to match target batch_size (8)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:47:57] Energy consumed for RAM : 0.000050 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 16:47:57] Energy consumed for all GPUs : 0.000030 kWh. Total GPU Power : 7.231286163651481 W\n",
      "[codecarbon INFO @ 16:47:57] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 16:47:57] 0.000257 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:12] Energy consumed for RAM : 0.000099 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 16:48:12] Energy consumed for all GPUs : 0.000062 kWh. Total GPU Power : 7.75361859759768 W\n",
      "[codecarbon INFO @ 16:48:12] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 16:48:12] 0.000516 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:27] Energy consumed for RAM : 0.000148 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 16:48:27] Energy consumed for all GPUs : 0.000097 kWh. Total GPU Power : 8.289617327029633 W\n",
      "[codecarbon INFO @ 16:48:27] Energy consumed for all CPUs : 0.000532 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 16:48:27] 0.000777 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:48:42] Energy consumed for RAM : 0.000198 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 16:48:42] Energy consumed for all GPUs : 0.000133 kWh. Total GPU Power : 8.527835716261542 W\n",
      "[codecarbon INFO @ 16:48:42] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 16:48:42] 0.001039 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:36:43] Energy consumed for RAM : 0.001484 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 15:36:43] Energy consumed for all GPUs : 0.001217 kWh. Total GPU Power : 6.149029723460931 W\n",
      "[codecarbon INFO @ 15:36:43] Energy consumed for all CPUs : 0.005317 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 15:36:43] 0.008018 kWh of electricity used since the beginning.\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:36:49] Energy consumed for RAM : 0.001732 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 15:36:49] Energy consumed for all GPUs : 0.001377 kWh. Total GPU Power : 7.0567263850462405 W\n",
      "[codecarbon INFO @ 15:36:49] Energy consumed for all CPUs : 0.006202 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 15:36:49] 0.009311 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:36:58] Energy consumed for RAM : 0.001534 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 15:36:58] Energy consumed for all GPUs : 0.001250 kWh. Total GPU Power : 7.9407787181463885 W\n",
      "[codecarbon INFO @ 15:36:58] Energy consumed for all CPUs : 0.005494 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 15:36:58] 0.008278 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:37:04] Energy consumed for RAM : 0.001781 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 15:37:04] Energy consumed for all GPUs : 0.001405 kWh. Total GPU Power : 6.673287926108866 W\n",
      "[codecarbon INFO @ 15:37:04] Energy consumed for all CPUs : 0.006380 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 15:37:04] 0.009566 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:37:13] Energy consumed for RAM : 0.001583 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 15:37:13] Energy consumed for all GPUs : 0.001283 kWh. Total GPU Power : 7.950922875693838 W\n",
      "[codecarbon INFO @ 15:37:13] Energy consumed for all CPUs : 0.005671 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 15:37:13] 0.008538 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:37:19] Energy consumed for RAM : 0.001831 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 15:37:19] Energy consumed for all GPUs : 0.001442 kWh. Total GPU Power : 8.973973699237916 W\n",
      "[codecarbon INFO @ 15:37:19] Energy consumed for all CPUs : 0.006557 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 15:37:19] 0.009829 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:37:28] Energy consumed for RAM : 0.001633 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 15:37:28] Energy consumed for all GPUs : 0.001320 kWh. Total GPU Power : 8.852517264382987 W\n",
      "[codecarbon INFO @ 15:37:28] Energy consumed for all CPUs : 0.005848 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 15:37:28] 0.008801 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:37:34] Energy consumed for RAM : 0.001880 kWh. RAM Power : 11.879536628723145 W\n",
      "[codecarbon INFO @ 15:37:34] Energy consumed for all GPUs : 0.001483 kWh. Total GPU Power : 9.87137052687312 W\n",
      "[codecarbon INFO @ 15:37:34] Energy consumed for all CPUs : 0.006734 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 15:37:34] 0.010097 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Making predictions on the test set\n",
    "preds = trainer.predict(tokenized_test_dataset).predictions.argmax(-1)\n",
    "f1 = f1_score(tokenized_test_dataset['label'], preds, average='macro')\n",
    "print(f\"Macro F1 Score: {f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

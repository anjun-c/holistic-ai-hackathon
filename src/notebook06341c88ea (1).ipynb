{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install codecarbon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:03:48.231169Z","iopub.execute_input":"2024-11-24T13:03:48.231523Z","iopub.status.idle":"2024-11-24T13:03:56.568263Z","shell.execute_reply.started":"2024-11-24T13:03:48.231495Z","shell.execute_reply":"2024-11-24T13:03:56.567011Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: codecarbon in /opt/conda/lib/python3.10/site-packages (2.7.4)\nRequirement already satisfied: arrow in /opt/conda/lib/python3.10/site-packages (from codecarbon) (1.3.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from codecarbon) (8.1.7)\nRequirement already satisfied: fief-client[cli] in /opt/conda/lib/python3.10/site-packages (from codecarbon) (0.20.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from codecarbon) (2.2.2)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from codecarbon) (0.20.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from codecarbon) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from codecarbon) (9.0.0)\nRequirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from codecarbon) (11.4.1)\nRequirement already satisfied: questionary in /opt/conda/lib/python3.10/site-packages (from codecarbon) (2.0.1)\nRequirement already satisfied: rapidfuzz in /opt/conda/lib/python3.10/site-packages (from codecarbon) (3.10.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from codecarbon) (2.32.3)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from codecarbon) (13.7.1)\nRequirement already satisfied: typer in /opt/conda/lib/python3.10/site-packages (from codecarbon) (0.12.3)\nRequirement already satisfied: python-dateutil>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from arrow->codecarbon) (2.9.0.post0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow->codecarbon) (2.9.0.20240316)\nRequirement already satisfied: httpx<0.28.0,>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (0.27.0)\nRequirement already satisfied: jwcrypto<2.0.0,>=1.4 in /opt/conda/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (1.5.6)\nRequirement already satisfied: yaspin in /opt/conda/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (3.1.0)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas->codecarbon) (1.26.4)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->codecarbon) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->codecarbon) (2024.1)\nRequirement already satisfied: prompt_toolkit<=3.0.36,>=2.0 in /opt/conda/lib/python3.10/site-packages (from questionary->codecarbon) (3.0.36)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->codecarbon) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->codecarbon) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->codecarbon) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->codecarbon) (2024.8.30)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->codecarbon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->codecarbon) (2.18.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from typer->codecarbon) (4.12.2)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer->codecarbon) (1.5.4)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\nRequirement already satisfied: cryptography>=3.4 in /opt/conda/lib/python3.10/site-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (42.0.8)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt_toolkit<=3.0.36,>=2.0->questionary->codecarbon) (0.2.13)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\nRequirement already satisfied: termcolor<2.4.0,>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from yaspin->fief-client[cli]->codecarbon) (2.3.0)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.16.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.2.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport random\nimport torch\nfrom datasets import Dataset\nfrom sklearn.metrics import f1_score\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    pipeline,\n)\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:03:56.570210Z","iopub.execute_input":"2024-11-24T13:03:56.570511Z","iopub.status.idle":"2024-11-24T13:03:56.575727Z","shell.execute_reply.started":"2024-11-24T13:03:56.570481Z","shell.execute_reply":"2024-11-24T13:03:56.574762Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Load the dataset\nsplits = {'train': 'train.csv', 'test': 'test.csv'}\ntrain_data = pd.read_csv(\"hf://datasets/holistic-ai/EMGSD/\" + splits[\"train\"])\ntest_data = pd.read_csv(\"hf://datasets/holistic-ai/EMGSD/\" + splits[\"test\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:03:56.576629Z","iopub.execute_input":"2024-11-24T13:03:56.576861Z","iopub.status.idle":"2024-11-24T13:03:57.460407Z","shell.execute_reply.started":"2024-11-24T13:03:56.576839Z","shell.execute_reply":"2024-11-24T13:03:57.459635Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Use a subset of the data for faster training\nsample_ratio = 0.001\ntrain_data = train_data.sample(frac=sample_ratio, random_state=42)\ntest_data = test_data.sample(frac=sample_ratio, random_state=42)\n\n# Prepare train and test sets by using both training and testing data\nX_train, y_train = train_data[\"text\"].values.tolist(), train_data[\"category\"].values.tolist()\nX_test, y_test = test_data[\"text\"].values.tolist(), test_data[\"category\"].values.tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:03:57.463258Z","iopub.execute_input":"2024-11-24T13:03:57.464065Z","iopub.status.idle":"2024-11-24T13:03:57.476192Z","shell.execute_reply.started":"2024-11-24T13:03:57.464019Z","shell.execute_reply":"2024-11-24T13:03:57.475320Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Function to compute Macro F1 score\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    f1 = f1_score(labels, predictions, average='macro')  # Use macro F1\n    return {\"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:03:57.477322Z","iopub.execute_input":"2024-11-24T13:03:57.477618Z","iopub.status.idle":"2024-11-24T13:03:57.488400Z","shell.execute_reply.started":"2024-11-24T13:03:57.477591Z","shell.execute_reply":"2024-11-24T13:03:57.487565Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Convert to Hugging Face dataset format\ntrain_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\ntest_dataset = Dataset.from_dict({\"text\": X_test, \"label\": y_test})\n\n# Map labels to IDs\nlabel2id = {\n    'stereotype': 0,\n    'unrelated': 1,\n    'neutral': 2,\n}\n\nid2label = {v: k for k, v in label2id.items()}\n\ndef map_labels(example):\n    example['label'] = label2id[example['label']]\n    return example\n\n# Apply the mapping to your dataset\ntrain_dataset = train_dataset.map(map_labels)\ntest_dataset = test_dataset.map(map_labels)\n\n# Random Model Prediction\nrandom.seed(42)\nrandom_predictions = [random.choice(y_test) for _ in range(len(y_test))]\n\n# Evaluate the model\nf1 = f1_score(y_test, random_predictions, average='macro')\nprint(f\"F1 Score: {f1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:03:57.489531Z","iopub.execute_input":"2024-11-24T13:03:57.490197Z","iopub.status.idle":"2024-11-24T13:03:57.551252Z","shell.execute_reply.started":"2024-11-24T13:03:57.490156Z","shell.execute_reply":"2024-11-24T13:03:57.550397Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/46 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57f1c43306984ab083d33c7a285da9ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eafa162c5efd440b9705ee3963b7c50f"}},"metadata":{}},{"name":"stdout","text":"F1 Score: 0.35714285714285715\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom codecarbon import EmissionsTracker\n\n# TF-IDF Vectorizer\nX_train = train_dataset['text']\ny_train = train_dataset['label']  \nX_test = test_dataset['text']\ny_test = test_dataset['label']\n\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Logistic Regression Model\nmodel = LogisticRegression()\n\n# Tracking emissions with CodeCarbon\ntracker = EmissionsTracker()\n#tracker.start()\n\n# Fit the model\nmodel.fit(X_train_tfidf, y_train)\n\n# Evaluate the model\npredictions = model.predict(X_test_tfidf)\nf1 = f1_score(y_test, predictions, average='macro')\n\n#emissions = tracker.stop()\nprint(f\"F1 Score: {f1}\")\n#print(f\"Training carbon emissions: {emissions} kg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:03:57.552238Z","iopub.execute_input":"2024-11-24T13:03:57.552498Z","iopub.status.idle":"2024-11-24T13:03:57.575746Z","shell.execute_reply.started":"2024-11-24T13:03:57.552472Z","shell.execute_reply":"2024-11-24T13:03:57.574975Z"}},"outputs":[{"name":"stderr","text":"[codecarbon ERROR @ 13:03:57] Error: Another instance of codecarbon is already running. Turn off the other instance to be able to run this one. Exiting.\n","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.17777777777777778\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nimport matplotlib.pyplot as plt\n\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n\n# Tokenization function\ndef tokenize_function(example):\n    return tokenizer(example['text'], padding='max_length', truncation=True)\n    \n# Apply the tokenizer to the dataset\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:03:57.576634Z","iopub.execute_input":"2024-11-24T13:03:57.576868Z","iopub.status.idle":"2024-11-24T13:03:57.801795Z","shell.execute_reply.started":"2024-11-24T13:03:57.576845Z","shell.execute_reply":"2024-11-24T13:03:57.800910Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/46 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a8aa80a05fa430eb5c5eea7e8c27b8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd186deb0867444097956cb303f902b6"}},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# Load pre-trained ALBERT model with classification head\n#model = AutoModelForSequenceClassification.from_pretrained(\n#    \"albert-base-v2\", \n#    num_labels=3, \n#    label2id=label2id,\n#    id2label=id2label\n#)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"roberta-base\",\n    num_labels=3,\n    label2id=label2id,\n    id2label=id2label\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # \"mps\" For macOS (Apple Silicon)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:03:57.802782Z","iopub.execute_input":"2024-11-24T13:03:57.803030Z","iopub.status.idle":"2024-11-24T13:03:58.167275Z","shell.execute_reply.started":"2024-11-24T13:03:57.803006Z","shell.execute_reply":"2024-11-24T13:03:58.166368Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# Fine-tuning the model and save the best model\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    eval_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    logging_dir='./logs',\n    num_train_epochs=3,\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_total_limit=1,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True\n)\n\ntrainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    compute_metrics=compute_metrics,  # Use macro F1 computation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:03:58.170856Z","iopub.execute_input":"2024-11-24T13:03:58.171355Z","iopub.status.idle":"2024-11-24T13:03:58.238515Z","shell.execute_reply.started":"2024-11-24T13:03:58.171328Z","shell.execute_reply":"2024-11-24T13:03:58.237577Z"}},"outputs":[{"name":"stderr","text":"[codecarbon ERROR @ 13:03:58] Error: Another instance of codecarbon is already running. Turn off the other instance to be able to run this one. Exiting.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Tracking emissions with CodeCarbon\ntracker = EmissionsTracker()\n#tracker.start()\n\n#trainer.train()\n\n#emissions = tracker.stop()\n\nprint()\n#print(f\"Training carbon emissions: {emissions} kg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:03:58.240254Z","iopub.execute_input":"2024-11-24T13:03:58.240496Z","iopub.status.idle":"2024-11-24T13:03:58.247111Z","shell.execute_reply.started":"2024-11-24T13:03:58.240472Z","shell.execute_reply":"2024-11-24T13:03:58.246245Z"}},"outputs":[{"name":"stderr","text":"[codecarbon ERROR @ 13:03:58] Error: Another instance of codecarbon is already running. Turn off the other instance to be able to run this one. Exiting.\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"!pip install nlpaug","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:03:58.248037Z","iopub.execute_input":"2024-11-24T13:03:58.248294Z","iopub.status.idle":"2024-11-24T13:04:06.976536Z","shell.execute_reply.started":"2024-11-24T13:03:58.248270Z","shell.execute_reply":"2024-11-24T13:04:06.975549Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (2.2.2)\nRequirement already satisfied: requests>=2.22.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (2.32.3)\nCollecting gdown>=4.0.0 (from nlpaug)\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (3.15.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.66.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (2024.8.30)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown, nlpaug\nSuccessfully installed gdown-5.2.0 nlpaug-1.1.11\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig,\n    Trainer,\n    TrainingArguments,\n    AdamW,\n    get_linear_schedule_with_warmup\n)\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.nn import CrossEntropyLoss\nfrom datasets import Dataset, concatenate_datasets\nimport pandas as pd\nimport re\nfrom collections import Counter\nfrom imblearn.over_sampling import RandomOverSampler\nimport nlpaug.augmenter.word as naw\nimport json\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nmodel_name = \"distilroberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nimport gc\nfrom transformers import TrainerCallback\n\nclass GpuMemoryCallback(TrainerCallback):\n    def on_epoch_end(self, args, state, control, **kwargs):\n        allocated_memory = torch.cuda.memory_allocated() / 1024**2\n        cached_memory = torch.cuda.memory_reserved() / 1024**2\n        print(f\"Epoch {state.epoch}:\")\n        print(f\"Allocated GPU Memory: {allocated_memory:.2f} MB\")\n        print(f\"Cached GPU Memory: {cached_memory:.2f} MB\")\n        print(torch.cuda.memory_summary())\n\nclass MemoryCleanupCallback(TrainerCallback):\n    def on_epoch_end(self, args, state, control, **kwargs):\n        print(\"Cleared cache\")\n        gc.collect()\n        torch.cuda.empty_cache()\n        torch.cuda.empty_cache()\n        torch.cuda.empty_cache()\n        torch.cuda.empty_cache()\n        torch.cuda.empty_cache()\n\n\ndef cleanup_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n# **1. Preprocess Text**\ndef preprocess_text(examples):\n    text = examples['text']\n    # Convert to lowercase\n    text = text.lower()\n    # Remove URLs\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n    # Remove special characters\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    examples['text'] = text\n    return examples\n\n# Apply preprocessing\ntrain_dataset = train_dataset.map(preprocess_text)\ntest_dataset = test_dataset.map(preprocess_text)\n\n# **2. Convert to Pandas DataFrame for Oversampling**\ntrain_df = train_dataset.to_pandas()\n\n# Separate features and labels\nX = train_df[['text']]\ny = train_df['label']\n\n# **3. Over-Sample Minority Classes**\nros = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = ros.fit_resample(X, y)\n\n# Convert back to Hugging Face Dataset\nresampled_df = pd.concat([X_resampled, y_resampled], axis=1)\nresampled_dataset = Dataset.from_pandas(resampled_df)\n\n# **4. Data Augmentation**\ndef augment_text(examples):\n    aug = naw.SynonymAug()\n    augmented_texts = [\" \".join(text) if isinstance(text, list) else text for text in examples['text']]\n    return {\"text\": augmented_texts, \"label\": examples['label']}\n\n# Apply augmentation\naugmented_dataset = resampled_dataset.map(augment_text, batched=True)\n\n# Combine with resampled dataset\ncombined_dataset = concatenate_datasets([resampled_dataset, augmented_dataset])\n\n# **5. Tokenization**\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['text'],\n        padding='max_length',\n        truncation=True,\n        max_length=100  # Adjust to a lower value if possible\n    )\n\n# Ensure that tokenized dataset includes 'input_ids' and other keys required for training\ntokenized_train_dataset = combined_dataset.map(tokenize_function, batched=True)\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Ensure the labels are correctly aligned in the dataset\ntokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\ntokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n\n# Set format to PyTorch tensors for compatibility with Trainer\ntokenized_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntokenized_test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# **6. Update Labels and Compute Class Weights**\ntrain_labels = np.array(tokenized_train_dataset['labels'])\n\n# Check class distribution\nlabel_counts = Counter(train_labels)\nprint(\"Class distribution after oversampling and augmentation:\", label_counts)\n\n# Compute class weights\nunique_classes = np.unique(train_labels).astype(int)\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=unique_classes,\n    y=train_labels\n)\nclass_weights = class_weights / class_weights.sum()\nclass_weights = torch.tensor(class_weights, dtype=torch.float)\n\n# **7. Determine Device**\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# **8. Load Config and Customize Model**\nlabel2id = {int(i): f'class_{i}' for i in unique_classes}\nid2label = {v: k for k, v in label2id.items()}\n\nconfig = AutoConfig.from_pretrained(model_name, num_labels=len(unique_classes), label2id=label2id, id2label=id2label)\n\nfrom transformers import RobertaForSequenceClassification\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nclass CustomRoberta(RobertaForSequenceClassification):\n    def __init__(self, config):\n        super().__init__(config)\n        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        outputs = self.roberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        sequence_output = outputs[0]  # [batch_size, seq_length, hidden_size]\n        pooled_output = sequence_output[:, 0, :]  # [batch_size, hidden_size]\n        logits = self.classifier(pooled_output)  # [batch_size, num_labels]\n\n        return SequenceClassifierOutput(\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\nmodel = CustomRoberta.from_pretrained(model_name, config=config)\nmodel = torch.nn.DataParallel(model)\nmodel.to(device)\nclass_weights = class_weights.to(device)\n\n# **9. Define Training Arguments**\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    num_train_epochs=5,\n    learning_rate=1e-5,\n    weight_decay=0.01,\n    warmup_steps=100,\n    logging_dir='./logs',\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_total_limit=1,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    fp16=False,  # Disable mixed precision to avoid errors with gradient scaling\n    remove_unused_columns=False  # Ensure that all columns are preserved during training\n)\n\n# **10. Initialize Optimizer and Scheduler**\noptimizer = AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=training_args.weight_decay)\n\ntotal_steps = (\n    len(tokenized_train_dataset) // training_args.per_device_train_batch_size\n) * training_args.num_train_epochs\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=training_args.warmup_steps,\n    num_training_steps=total_steps\n)\n\n# **11. Define Custom Trainer Class**\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\").to(device)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        # Compute custom loss with class weights on the correct device\n        loss_fct = CrossEntropyLoss(weight=class_weights.to(device))\n        loss = loss_fct(logits, labels)\n        allocated_memory = torch.cuda.memory_allocated() / 1024**2\n        reserved_memory = torch.cuda.memory_reserved() / 1024**2\n        print(f\"Allocated Memory: {allocated_memory:.2f} MB\")\n        print(f\"Reserved Memory: {reserved_memory:.2f} MB\")\n        cleanup_memory()\n        \n        if return_outputs:\n            return (loss, outputs)\n        else:\n            return loss\n\n    def training_step(self, model, inputs, num_items_in_batch=None):\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n        loss = self.compute_loss(model, inputs)\n        allocated_memory = torch.cuda.memory_allocated() / 1024**2\n        reserved_memory = torch.cuda.memory_reserved() / 1024**2\n        print(f\"Allocated Memory: {allocated_memory:.2f} MB\")\n        print(f\"Reserved Memory: {reserved_memory:.2f} MB\")\n        cleanup_memory()\n        \n        return loss\n\n# **12. Define Compute Metrics Function**\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\n        'accuracy': accuracy_score(p.label_ids, preds),\n        'f1': f1_score(p.label_ids, preds, average='macro')\n    }\n\n# **13. Initialize the Custom Trainer**\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    optimizers=(optimizer, scheduler),\n    callbacks=[GpuMemoryCallback(), MemoryCleanupCallback()]\n)\n\n# Start training\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:45:11.747437Z","iopub.execute_input":"2024-11-24T13:45:11.747769Z","iopub.status.idle":"2024-11-24T13:45:16.945247Z","shell.execute_reply.started":"2024-11-24T13:45:11.747740Z","shell.execute_reply":"2024-11-24T13:45:16.941170Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/46 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b970ec1249314c54a64ca03ed9fcb2f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e66eb467ac41499ebf7628ee1841a002"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/54 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c633897e5884fe186fb20f922ca08b9"}},"metadata":{}},{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/108 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d711457e99204344998b438b177386cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b65e0ebc118d48bb8c52a3b131f795d1"}},"metadata":{}},{"name":"stdout","text":"Class distribution after oversampling and augmentation: Counter({2: 36, 1: 36, 0: 36})\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CustomRoberta were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n[codecarbon ERROR @ 13:45:12] Error: Another instance of codecarbon is already running. Turn off the other instance to be able to run this one. Exiting.\n[codecarbon WARNING @ 13:45:12] Another instance of codecarbon is already running. Exiting.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"name":"stdout","text":"Allocated Memory: 2128.75 MB\nReserved Memory: 2296.00 MB\nAllocated Memory: 2128.75 MB\nReserved Memory: 2276.00 MB\nAllocated Memory: 2249.75 MB\nReserved Memory: 2296.00 MB\nAllocated Memory: 2249.75 MB\nReserved Memory: 2296.00 MB\nAllocated Memory: 2361.13 MB\nReserved Memory: 2370.00 MB\nAllocated Memory: 2361.13 MB\nReserved Memory: 2368.00 MB\nAllocated Memory: 2464.50 MB\nReserved Memory: 2482.00 MB\nAllocated Memory: 2464.50 MB\nReserved Memory: 2480.00 MB\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[124], line 264\u001b[0m\n\u001b[1;32m    252\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m    253\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    254\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[GpuMemoryCallback(), MemoryCleanupCallback()]\n\u001b[1;32m    261\u001b[0m )\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2434\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2429\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   2430\u001b[0m         amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer),\n\u001b[1;32m   2431\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   2432\u001b[0m     )\n\u001b[1;32m   2433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2434\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2436\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2440\u001b[0m     is_accelerate_available()\n\u001b[1;32m   2441\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2442\u001b[0m ):\n\u001b[1;32m   2443\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2346\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2344\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m parameters \u001b[38;5;241m==\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()]:\n\u001b[1;32m   2345\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mclip_grad_norm_(max_norm, norm_type)\n\u001b[0;32m-> 2346\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2290\u001b[0m, in \u001b[0;36mAccelerator.unscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(opt, AcceleratedOptimizer):\n\u001b[1;32m   2289\u001b[0m     opt \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39moptimizer\n\u001b[0;32m-> 2290\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:322\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enabled:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_scale_growth_tracker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munscale_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m optimizer_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_per_optimizer_states[\u001b[38;5;28mid\u001b[39m(optimizer)]\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState\u001b[38;5;241m.\u001b[39mUNSCALED:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:158\u001b[0m, in \u001b[0;36mGradScaler._check_scale_growth_tracker\u001b[0;34m(self, funcname)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_scale_growth_tracker\u001b[39m(\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m, funcname: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    157\u001b[0m     fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may indicate your script did not use scaler.scale(loss or outputs) earlier in the iteration.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but _scale is None.  \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fix\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_growth_tracker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but _growth_tracker is None.  \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fix\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_growth_tracker)\n","\u001b[0;31mAssertionError\u001b[0m: Attempted unscale_ but _scale is None.  This may indicate your script did not use scaler.scale(loss or outputs) earlier in the iteration."],"ename":"AssertionError","evalue":"Attempted unscale_ but _scale is None.  This may indicate your script did not use scaler.scale(loss or outputs) earlier in the iteration.","output_type":"error"}],"execution_count":124},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:30:48.337251Z","iopub.execute_input":"2024-11-24T13:30:48.337599Z","iopub.status.idle":"2024-11-24T13:30:48.343189Z","shell.execute_reply.started":"2024-11-24T13:30:48.337567Z","shell.execute_reply":"2024-11-24T13:30:48.342322Z"}},"outputs":[],"execution_count":115},{"cell_type":"code","source":"# Start training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:41:13.642767Z","iopub.execute_input":"2024-11-24T13:41:13.643134Z","iopub.status.idle":"2024-11-24T13:41:18.363333Z","shell.execute_reply.started":"2024-11-24T13:41:13.643082Z","shell.execute_reply":"2024-11-24T13:41:18.361994Z"}},"outputs":[{"name":"stderr","text":"[codecarbon WARNING @ 13:41:14] Another instance of codecarbon is already running. Exiting.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"name":"stdout","text":"Allocated Memory: 1714.74 MB\nReserved Memory: 1798.00 MB\nAllocated Memory: 1714.74 MB\nReserved Memory: 1798.00 MB\nAllocated Memory: 1835.74 MB\nReserved Memory: 1876.00 MB\nAllocated Memory: 1835.74 MB\nReserved Memory: 1874.00 MB\nAllocated Memory: 1946.24 MB\nReserved Memory: 1954.00 MB\nAllocated Memory: 1946.24 MB\nReserved Memory: 1954.00 MB\nAllocated Memory: 2050.49 MB\nReserved Memory: 2066.00 MB\nAllocated Memory: 2050.49 MB\nReserved Memory: 2064.00 MB\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[122], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2434\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2429\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   2430\u001b[0m         amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer),\n\u001b[1;32m   2431\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   2432\u001b[0m     )\n\u001b[1;32m   2433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2434\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2436\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2440\u001b[0m     is_accelerate_available()\n\u001b[1;32m   2441\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2442\u001b[0m ):\n\u001b[1;32m   2443\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2346\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2344\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m parameters \u001b[38;5;241m==\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()]:\n\u001b[1;32m   2345\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mclip_grad_norm_(max_norm, norm_type)\n\u001b[0;32m-> 2346\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2290\u001b[0m, in \u001b[0;36mAccelerator.unscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(opt, AcceleratedOptimizer):\n\u001b[1;32m   2289\u001b[0m     opt \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39moptimizer\n\u001b[0;32m-> 2290\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:322\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enabled:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_scale_growth_tracker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munscale_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m optimizer_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_per_optimizer_states[\u001b[38;5;28mid\u001b[39m(optimizer)]\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState\u001b[38;5;241m.\u001b[39mUNSCALED:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:158\u001b[0m, in \u001b[0;36mGradScaler._check_scale_growth_tracker\u001b[0;34m(self, funcname)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_scale_growth_tracker\u001b[39m(\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m, funcname: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    157\u001b[0m     fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may indicate your script did not use scaler.scale(loss or outputs) earlier in the iteration.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but _scale is None.  \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fix\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_growth_tracker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but _growth_tracker is None.  \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fix\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_growth_tracker)\n","\u001b[0;31mAssertionError\u001b[0m: Attempted unscale_ but _scale is None.  This may indicate your script did not use scaler.scale(loss or outputs) earlier in the iteration."],"ename":"AssertionError","evalue":"Attempted unscale_ but _scale is None.  This may indicate your script did not use scaler.scale(loss or outputs) earlier in the iteration.","output_type":"error"}],"execution_count":122},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n# Making predictions on the test set\npreds = trainer.predict(tokenized_test_dataset).predictions.argmax(-1)\nf1 = f1_score(tokenized_test_dataset['label'], preds, average='macro')\nprint(f\"Macro F1 Score: {f1:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:04:54.430872Z","iopub.status.idle":"2024-11-24T13:04:54.431190Z","shell.execute_reply.started":"2024-11-24T13:04:54.431026Z","shell.execute_reply":"2024-11-24T13:04:54.431041Z"}},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install codecarbon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:01:58.782859Z","iopub.execute_input":"2024-11-24T13:01:58.783699Z","iopub.status.idle":"2024-11-24T13:02:06.976091Z","shell.execute_reply.started":"2024-11-24T13:01:58.783662Z","shell.execute_reply":"2024-11-24T13:02:06.975170Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: codecarbon in /opt/conda/lib/python3.10/site-packages (2.7.4)\nRequirement already satisfied: arrow in /opt/conda/lib/python3.10/site-packages (from codecarbon) (1.3.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from codecarbon) (8.1.7)\nRequirement already satisfied: fief-client[cli] in /opt/conda/lib/python3.10/site-packages (from codecarbon) (0.20.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from codecarbon) (2.2.2)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from codecarbon) (0.20.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from codecarbon) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from codecarbon) (9.0.0)\nRequirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from codecarbon) (11.4.1)\nRequirement already satisfied: questionary in /opt/conda/lib/python3.10/site-packages (from codecarbon) (2.0.1)\nRequirement already satisfied: rapidfuzz in /opt/conda/lib/python3.10/site-packages (from codecarbon) (3.10.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from codecarbon) (2.32.3)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from codecarbon) (13.7.1)\nRequirement already satisfied: typer in /opt/conda/lib/python3.10/site-packages (from codecarbon) (0.12.3)\nRequirement already satisfied: python-dateutil>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from arrow->codecarbon) (2.9.0.post0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow->codecarbon) (2.9.0.20240316)\nRequirement already satisfied: httpx<0.28.0,>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (0.27.0)\nRequirement already satisfied: jwcrypto<2.0.0,>=1.4 in /opt/conda/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (1.5.6)\nRequirement already satisfied: yaspin in /opt/conda/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (3.1.0)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas->codecarbon) (1.26.4)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->codecarbon) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->codecarbon) (2024.1)\nRequirement already satisfied: prompt_toolkit<=3.0.36,>=2.0 in /opt/conda/lib/python3.10/site-packages (from questionary->codecarbon) (3.0.36)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->codecarbon) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->codecarbon) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->codecarbon) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->codecarbon) (2024.8.30)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->codecarbon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->codecarbon) (2.18.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from typer->codecarbon) (4.12.2)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer->codecarbon) (1.5.4)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\nRequirement already satisfied: cryptography>=3.4 in /opt/conda/lib/python3.10/site-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (42.0.8)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt_toolkit<=3.0.36,>=2.0->questionary->codecarbon) (0.2.13)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\nRequirement already satisfied: termcolor<2.4.0,>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from yaspin->fief-client[cli]->codecarbon) (2.3.0)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.16.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.2.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport random\nimport torch\nfrom datasets import Dataset\nfrom sklearn.metrics import f1_score\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    pipeline,\n)\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:06.978314Z","iopub.execute_input":"2024-11-24T13:02:06.978712Z","iopub.status.idle":"2024-11-24T13:02:06.984166Z","shell.execute_reply.started":"2024-11-24T13:02:06.978670Z","shell.execute_reply":"2024-11-24T13:02:06.983354Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Load the dataset\nsplits = {'train': 'train.csv', 'test': 'test.csv'}\ntrain_data = pd.read_csv(\"hf://datasets/holistic-ai/EMGSD/\" + splits[\"train\"])\ntest_data = pd.read_csv(\"hf://datasets/holistic-ai/EMGSD/\" + splits[\"test\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:06.985135Z","iopub.execute_input":"2024-11-24T13:02:06.985405Z","iopub.status.idle":"2024-11-24T13:02:08.346236Z","shell.execute_reply.started":"2024-11-24T13:02:06.985381Z","shell.execute_reply":"2024-11-24T13:02:08.345466Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Use a subset of the data for faster training\nsample_ratio = 0.001\ntrain_data = train_data.sample(frac=sample_ratio, random_state=42)\ntest_data = test_data.sample(frac=sample_ratio, random_state=42)\n\n# Prepare train and test sets by using both training and testing data\nX_train, y_train = train_data[\"text\"].values.tolist(), train_data[\"category\"].values.tolist()\nX_test, y_test = test_data[\"text\"].values.tolist(), test_data[\"category\"].values.tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:08.349028Z","iopub.execute_input":"2024-11-24T13:02:08.349471Z","iopub.status.idle":"2024-11-24T13:02:08.363498Z","shell.execute_reply.started":"2024-11-24T13:02:08.349427Z","shell.execute_reply":"2024-11-24T13:02:08.362429Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Function to compute Macro F1 score\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    f1 = f1_score(labels, predictions, average='macro')  # Use macro F1\n    return {\"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:08.364607Z","iopub.execute_input":"2024-11-24T13:02:08.364866Z","iopub.status.idle":"2024-11-24T13:02:08.374611Z","shell.execute_reply.started":"2024-11-24T13:02:08.364842Z","shell.execute_reply":"2024-11-24T13:02:08.373868Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Convert to Hugging Face dataset format\ntrain_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\ntest_dataset = Dataset.from_dict({\"text\": X_test, \"label\": y_test})\n\n# Map labels to IDs\nlabel2id = {\n    'stereotype': 0,\n    'unrelated': 1,\n    'neutral': 2,\n}\n\nid2label = {v: k for k, v in label2id.items()}\n\ndef map_labels(example):\n    example['label'] = label2id[example['label']]\n    return example\n\n# Apply the mapping to your dataset\ntrain_dataset = train_dataset.map(map_labels)\ntest_dataset = test_dataset.map(map_labels)\n\n# Random Model Prediction\nrandom.seed(42)\nrandom_predictions = [random.choice(y_test) for _ in range(len(y_test))]\n\n# Evaluate the model\nf1 = f1_score(y_test, random_predictions, average='macro')\nprint(f\"F1 Score: {f1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:08.375773Z","iopub.execute_input":"2024-11-24T13:02:08.376324Z","iopub.status.idle":"2024-11-24T13:02:08.434550Z","shell.execute_reply.started":"2024-11-24T13:02:08.376287Z","shell.execute_reply":"2024-11-24T13:02:08.433728Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/46 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf099371128b494682d54a00c826f1fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71f3364308f94c48969b65d94d612e77"}},"metadata":{}},{"name":"stdout","text":"F1 Score: 0.35714285714285715\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom codecarbon import EmissionsTracker\n\n# TF-IDF Vectorizer\nX_train = train_dataset['text']\ny_train = train_dataset['label']  \nX_test = test_dataset['text']\ny_test = test_dataset['label']\n\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Logistic Regression Model\nmodel = LogisticRegression()\n\n# Tracking emissions with CodeCarbon\ntracker = EmissionsTracker()\n#tracker.start()\n\n# Fit the model\nmodel.fit(X_train_tfidf, y_train)\n\n# Evaluate the model\npredictions = model.predict(X_test_tfidf)\nf1 = f1_score(y_test, predictions, average='macro')\n\n#emissions = tracker.stop()\nprint(f\"F1 Score: {f1}\")\nprint(f\"Training carbon emissions: {emissions} kg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:08.435662Z","iopub.execute_input":"2024-11-24T13:02:08.435933Z","iopub.status.idle":"2024-11-24T13:02:08.508814Z","shell.execute_reply.started":"2024-11-24T13:02:08.435907Z","shell.execute_reply":"2024-11-24T13:02:08.507477Z"}},"outputs":[{"name":"stderr","text":"[codecarbon ERROR @ 13:02:08] Error: Another instance of codecarbon is already running. Turn off the other instance to be able to run this one. Exiting.\n","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.17777777777777778\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#emissions = tracker.stop()\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining carbon emissions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43memissions\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m kg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'emissions' is not defined"],"ename":"NameError","evalue":"name 'emissions' is not defined","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nimport matplotlib.pyplot as plt\n\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n\n# Tokenization function\ndef tokenize_function(example):\n    return tokenizer(example['text'], padding='max_length', truncation=True)\n    \n# Apply the tokenizer to the dataset\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:08.509390Z","iopub.status.idle":"2024-11-24T13:02:08.509676Z","shell.execute_reply.started":"2024-11-24T13:02:08.509538Z","shell.execute_reply":"2024-11-24T13:02:08.509553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load pre-trained ALBERT model with classification head\n#model = AutoModelForSequenceClassification.from_pretrained(\n#    \"albert-base-v2\", \n#    num_labels=3, \n#    label2id=label2id,\n#    id2label=id2label\n#)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"roberta-base\",\n    num_labels=3,\n    label2id=label2id,\n    id2label=id2label\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # \"mps\" For macOS (Apple Silicon)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:08.510804Z","iopub.status.idle":"2024-11-24T13:02:08.511098Z","shell.execute_reply.started":"2024-11-24T13:02:08.510957Z","shell.execute_reply":"2024-11-24T13:02:08.510972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine-tuning the model and save the best model\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    eval_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    logging_dir='./logs',\n    num_train_epochs=3,\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_total_limit=1,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True\n)\n\ntrainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    compute_metrics=compute_metrics,  # Use macro F1 computation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:08.512096Z","iopub.status.idle":"2024-11-24T13:02:08.512441Z","shell.execute_reply.started":"2024-11-24T13:02:08.512297Z","shell.execute_reply":"2024-11-24T13:02:08.512313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tracking emissions with CodeCarbon\ntracker = EmissionsTracker()\n#tracker.start()\n\n#trainer.train()\n\n#emissions = tracker.stop()\n\nprint()\nprint(f\"Training carbon emissions: {emissions} kg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:08.515255Z","iopub.status.idle":"2024-11-24T13:02:08.515695Z","shell.execute_reply.started":"2024-11-24T13:02:08.515469Z","shell.execute_reply":"2024-11-24T13:02:08.515491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install nlpaug","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:08.516645Z","iopub.status.idle":"2024-11-24T13:02:08.517061Z","shell.execute_reply.started":"2024-11-24T13:02:08.516844Z","shell.execute_reply":"2024-11-24T13:02:08.516866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig,\n    Trainer,\n    TrainingArguments,\n    AdamW,\n    get_linear_schedule_with_warmup\n)\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.nn import CrossEntropyLoss\nfrom datasets import Dataset, concatenate_datasets\nimport pandas as pd\nimport re\nfrom collections import Counter\nfrom imblearn.over_sampling import RandomOverSampler\nimport nlpaug.augmenter.word as naw\n\nmodel_name = \"distilroberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nconfig = AutoConfig.from_pretrained(model_name, num_labels=3, label2id=label2id, id2label=id2label)\n\nimport gc\nfrom transformers import TrainerCallback\n\nclass MemoryCleanupCallback(TrainerCallback):\n    def on_epoch_end(self, args, state, control, **kwargs):\n        gc.collect()\n        torch.cuda.empty_cache()\n\n\n# **1. Preprocess Text**\ndef preprocess_text(examples):\n    text = examples['text']\n    # Convert to lowercase\n    text = text.lower()\n    # Remove URLs\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n    # Remove special characters\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    examples['text'] = text\n    return examples\n\n# Apply preprocessing\ntrain_dataset = train_dataset.map(preprocess_text)\ntest_dataset = test_dataset.map(preprocess_text)\n\n# **2. Convert to Pandas DataFrame for Oversampling**\ntrain_df = train_dataset.to_pandas()\n\n# Separate features and labels\nX = train_df[['text']]\ny = train_df['label']\n\n# **3. Over-Sample Minority Classes**\nros = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = ros.fit_resample(X, y)\n\n# Convert back to Hugging Face Dataset\nresampled_df = pd.concat([X_resampled, y_resampled], axis=1)\nresampled_dataset = Dataset.from_pandas(resampled_df)\n\n# **4. Data Augmentation**\ndef augment_text(examples):\n    aug = naw.SynonymAug()\n    augmented_texts = [\" \".join(text) if isinstance(text, list) else text for text in examples['text']]\n    return {\"text\": augmented_texts, \"label\": examples['label']}\n\n# Apply augmentation\naugmented_dataset = resampled_dataset.map(augment_text, batched=True)\n\n# Combine with resampled dataset\ncombined_dataset = concatenate_datasets([resampled_dataset, augmented_dataset])\n\n# **5. Tokenization**\n#tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['text'],\n        padding='max_length',\n        truncation=True,\n        max_length=100  # Adjust to a lower value if possible\n    )\n\n\ntokenized_train_dataset = combined_dataset.map(tokenize_function, batched=True)\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# **6. Update Labels and Compute Class Weights**\ntrain_labels = tokenized_train_dataset['label']\n\n# Check class distribution\nlabel_counts = Counter(train_labels)\nprint(\"Class distribution after oversampling and augmentation:\", label_counts)\n\n# Compute class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = class_weights / class_weights.sum()\nclass_weights = torch.tensor(class_weights, dtype=torch.float)\n\n# **7. Determine Device**\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# **8. Load Config and Customize Model**\nlabel2id = {0: 'class_0', 1: 'class_1', 2: 'class_2'}\nid2label = {v: k for k, v in label2id.items()}\n\n#config = AutoConfig.from_pretrained(\n#    \"roberta-large\",\n#    num_labels=3,\n#    label2id=label2id,\n#    id2label=id2label\n#)\n\nfrom transformers import RobertaForSequenceClassification\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nclass CustomRoberta(RobertaForSequenceClassification):\n    def __init__(self, config):\n        super().__init__(config)\n        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        outputs = self.roberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        sequence_output = outputs[0]  # [batch_size, seq_length, hidden_size]\n        pooled_output = sequence_output[:, 0, :]  # [batch_size, hidden_size]\n        logits = self.classifier(pooled_output)  # [batch_size, num_labels]\n\n        return SequenceClassifierOutput(\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n\n\n\n#model = CustomRoberta.from_pretrained(\"roberta-large\", config=config)\nmodel = CustomRoberta.from_pretrained(model_name, config=config)\nmodel.to(device)\nclass_weights = class_weights.to(device)\n\n# **9. Define Training Arguments**\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    num_train_epochs=5,\n    learning_rate=1e-5,\n    weight_decay=0.01,\n    warmup_steps=100,\n    logging_dir='./logs',\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_total_limit=1,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    fp16=torch.cuda.is_available(),\n)\n\n# **10. Initialize Optimizer and Scheduler**\noptimizer = AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=training_args.weight_decay)\n\ntotal_steps = (\n    len(tokenized_train_dataset) // training_args.per_device_train_batch_size\n) * training_args.num_train_epochs\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=training_args.warmup_steps,\n    num_training_steps=total_steps\n)\n\n# **11. Define Custom Trainer Class**\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\").to(device)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        # Compute custom loss with class weights on the correct device\n        loss_fct = CrossEntropyLoss(weight=class_weights.to(device))\n        loss = loss_fct(logits, labels)\n\n        if return_outputs:\n            return (loss, outputs)\n        else:\n            return loss\n\n    def training_step(self, model, inputs, num_items_in_batch=None):\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n        loss = self.compute_loss(model, inputs)\n        return loss\n\n# **12. Define Compute Metrics Function**\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\n        'accuracy': accuracy_score(p.label_ids, preds),\n        'f1': f1_score(p.label_ids, preds, average='macro')\n    }\n\n# **13. Initialize the Custom Trainer**\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    optimizers=(optimizer, scheduler),\n    callbacks=[MemoryCleanupCallback]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:08.519027Z","iopub.status.idle":"2024-11-24T13:02:08.519675Z","shell.execute_reply.started":"2024-11-24T13:02:08.519432Z","shell.execute_reply":"2024-11-24T13:02:08.519455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Start training\ntorch.cuda.empty_cache()\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:08.520621Z","iopub.status.idle":"2024-11-24T13:02:08.521036Z","shell.execute_reply.started":"2024-11-24T13:02:08.520820Z","shell.execute_reply":"2024-11-24T13:02:08.520841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n# Making predictions on the test set\npreds = trainer.predict(tokenized_test_dataset).predictions.argmax(-1)\nf1 = f1_score(tokenized_test_dataset['label'], preds, average='macro')\nprint(f\"Macro F1 Score: {f1:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:02:08.522332Z","iopub.status.idle":"2024-11-24T13:02:08.522629Z","shell.execute_reply.started":"2024-11-24T13:02:08.522488Z","shell.execute_reply":"2024-11-24T13:02:08.522503Z"}},"outputs":[],"execution_count":null}]}
